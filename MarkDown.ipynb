{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ุญููุ ุญุงูุง ฺฉู ุฏุชุง ุฑู ูพุฏุง ฺฉุฑุฏุ ุจุฑู ูุฑุญูู ุจู ูุฑุญูู ุจุจูู ฺฺฉุงุฑ ุจุงุฏ ุจฺฉู. ุฌูุฑ ุชูุถุญ ูุฏู ฺฉู ุงูฺฏุงุฑ ุฏุงุฑ ู ุจุงุฒ ูุฑุญููโุง ุงูุฌุงู ูุฏ ู ูุฑ ูุฑุญูู ุฑู ุจุงุฏ ุฏุฑุณุช ุฑุฏ ฺฉู ุชุง ุจุฑ ุจุนุฏ.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ๐ **ูุฑุญูู ฑ: ุจุฑุฑุณ ุงููู ุฏุชุงุณุช** (ูุซู ูฺฏุงู ฺฉุฑุฏู ุจู ููุดู ูุจู ุงุฒ ุดุฑูุน ุจุงุฒ)  \n",
    "ูุจู ุงุฒ ุงูฺฉู ูุฑ ฺฉุงุฑ ุจฺฉูุ ุจุงุฏ ู ูฺฏุงู ฺฉู ุจู ุฏุชุงุณุชุช ุจูุฏุงุฒ. ุงู ฺฉุงุฑ ฺฉูฺฉ ูโฺฉูู ฺฉู ุจููู:  \n",
    "- **ฺูุฏ ุชุง ุฏุงุฏู ุฏุงุฑุ** (ุฎุจ ฺฏูุช ฑณ,ฐฐฐ ุชุงุ ุงูฺฉ)  \n",
    "- **ุณุชููโูุง ุฏุชุงุณุช ฺ ูุณุชูุ** (ูุซูุงูุ ุณุชูู ูุชู ุชูุชุ ุณุชูู ุจุฑฺุณุจ ุงุญุณุงุณุงุช ู ...)  \n",
    "- **ููุฏุงุฑ ุฏุงุฏูโูุง ุฏุฑ ูุฑ ฺฉูุงุณ ุงุญุณุงุณุงุช ฺุทูุฑูุ** (ูุซูุงูุ ูฺฉูู นฐูช ุชูุชโูุง \"ุฎูุดุญุงู\" ุจุงุดู ู ููุท ฑฐูช ุจููุ ุงู ุจุงุนุซ ุนุฏู ุชุนุงุฏู ูุดู)  \n",
    "\n",
    "๐ **ฺฉุฏ ูพุดููุงุฏ ุจุฑุง ุจุฑุฑุณ ุงููู:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # ุงูู ุจุง ุงุณู ูุงูุช ุนูุถ ฺฉู\n",
    "print(df.head())  # ููุงุด ต ุณุทุฑ ุงูู\n",
    "print(df.info())  # ููุงุด ุงุทูุงุนุงุช ฺฉู ุฏุฑุจุงุฑู ุฏุชุงุณุช\n",
    "print(df['label'].value_counts())  # ุจุฑุฑุณ ุชูุฒุน ุจุฑฺุณุจโูุง\n",
    "```\n",
    "โ **ุงฺฏุฑ ุชูุฒุน ุฏุงุฏูโูุง ูุงูุชุนุงุฏู ุจูุฏุ ุจุงุฏ ุฑูุดโูุง ูุซู oversampling ุง undersampling ุฑู ุงูุฌุงู ุจุฏ (ุจุนุฏุงู ุจูุด ูโุฑุณู).**  \n",
    "\n",
    "๐ **ููุจุน ูพุดููุงุฏ ุจุฑุง ุจุฑุฑุณ ุงููู ุฏุชุงุณุช:**  \n",
    "[ุฑุงูููุง ุจุฑุฑุณ ุงููู ุฏุชุง ุฏุฑ ูพุงูุฏุงุณ](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## ๐ **ูุฑุญูู ฒ: ูพุงฺฉุณุงุฒ ุฏุงุฏูโูุง (Data Cleaning)** (ูุซู ุชูุฒ ฺฉุฑุฏู ุตูุญู ุจุงุฒ ูุจู ุงุฒ ุดุฑูุน)  \n",
    "ุจุงุฏ ูุทูุฆู ุจุด ฺฉู ุฏุชุง ุฎุงูุช ูุดฺฉู ุฎุงุต ูุฏุงุฑู. ูุดฺฉูุงุช ุฑุงุฌ ุงูุง ูุณุชู:  \n",
    "โ **ููุงุฏุฑ ุฎุงู (Missing Values)**  \n",
    "โ **ุฏุงุฏูโูุง ุชฺฉุฑุงุฑ (Duplicate Data)**  \n",
    "โ **ุชูุชโูุง ุจโูุนู (ูุซูุงู ุดุงูู ููุท @ ู # ู ููฺฉโูุง ุจุฏูู ูุชู ููุฏ)**  \n",
    "\n",
    "๐ **ฺฉุฏ ูพุดููุงุฏ ุจุฑุง ูพุงฺฉุณุงุฒ:**  \n",
    "```python\n",
    "# ุญุฐู ููุงุฏุฑ ุฎุงู\n",
    "df = df.dropna()\n",
    "\n",
    "# ุญุฐู ููุงุฏุฑ ุชฺฉุฑุงุฑ\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# ุจุฑุฑุณ ูููููโูุง ฺฉู ููุท ุดุงูู ููฺฉ ู ููุดู ูุณุชู\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "๐ **ููุจุน ูพุดููุงุฏ:**  \n",
    "[ุขููุฒุด Data Cleaning ุฏุฑ ูพุงูุฏุงุณ](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ๐ **ูุฑุญูู ณ: ูพุดโูพุฑุฏุงุฒุด ูุชู (Text Preprocessing)** (ูุซู ุชูุธูุงุช ุงููู ฺฉุงุฑุงฺฉุชุฑ ุชู ุจุงุฒ)  \n",
    "ุงูุงู ุจุงุฏ ูุชูโูุง ุชูุช ุฑู ุจุฑุง ูุฏู ุขูุงุฏู ฺฉูู. ุชูุชโูุง ูุนูููุงู ฺฉุซู ูุณุชู ู ูพุฑ ุงุฒ ฺุฒูุง ุงุถุงู ูุซู:  \n",
    "โ ููฺฉโูุง (https://...)  \n",
    "โ ูุงูโูุง ฺฉุงุฑุจุฑ (@user)  \n",
    "โ ูุดุชฺฏโูุง (#ููุถูุน)  \n",
    "โ ุงููุฌโูุง (๐๐กโค๏ธ)  \n",
    "โ ุญุฑูู ุงุถุงู ฺฉุดุฏู (\"ุนุงุงุงุงู\")  \n",
    "\n",
    "๐ **ฺฉุฏ ูพุดููุงุฏ ุจุฑุง ุชูุฒ ฺฉุฑุฏู ูุชู:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # ุญุฐู ููฺฉโูุง\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # ุญุฐู ููุดูโูุง\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # ุญุฐู ูุดุชฺฏโูุง\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # ุญุฐู \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # ุญุฐู ูุงุตููโูุง ุงุถุงู\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "๐ **ููุจุน ูพุดููุงุฏ:**  \n",
    "[ุฑุงูููุง ูพุดโูพุฑุฏุงุฒุด ูุชู](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## ๐ **ูุฑุญูู ด: ุชุญูู ุฏุงุฏูโูุง (Exploratory Data Analysis - EDA)** (ูุซู ุจุฑุฑุณ ูฺฺฏโูุง ุจุงุฒ ูุจู ุงุฒ ุดุฑูุน)  \n",
    "ุจุงุฏ ู ฺฉู ุฑู ุฏุชุงุช ุขูุงูุฒ ุงูุฌุงู ุจุฏ ฺฉู ุจููู:  \n",
    "- **ฺฉุฏูู ฺฉููุงุช ุจุดุชุฑ ุงุณุชูุงุฏู ุดุฏูุ**  \n",
    "- **ุทูู ุฌููุงุช ฺูุฏุฑูุ**  \n",
    "- **ุงุญุณุงุณุงุช ฺูุฏุฑ ุชูุฒุน ุดุฏูุ**  \n",
    "\n",
    "๐ **ฺฉุฏ ูพุดููุงุฏ ุจุฑุง ุจุฑุฑุณ ฺฉููุงุช ูพุฑ ุชฺฉุฑุงุฑ:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "๐ **ููุจุน ูพุดููุงุฏ:**  \n",
    "[EDA ุฏุฑ NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## ๐ข **ูุฑุญูู ต: ุชุจุฏู ูุชู ุจู ุนุฏุฏ (Tokenization & Embedding)** (ูุซู ุขูุงุฏู ฺฉุฑุฏู ฺฉุงุฑุงฺฉุชุฑูุง ุจุงุฒ ุจุฑุง ุญุฑฺฉุช)  \n",
    "ูุฏูโูุง ุงุฏฺฏุฑ ูุงุดู ููุท ุนุฏุฏ ูโููููุ ูพุณ ุจุงุฏ ูุชู ุฑู ุจู ุนุฏุฏ ุชุจุฏู ฺฉูู. ุฑูุดโูุง ูุฎุชูู ูุฌูุฏ ุฏุงุฑู:  \n",
    "- **Bag of Words (BoW)** โ ูุฏู ุณุงุฏูโุง ฺฉู ููุท ุชุนุฏุงุฏ ฺฉููุงุช ุฑู ูโุดูุฑู  \n",
    "- **TF-IDF** โ ููุฏุงุฑ ุงููุช ูุฑ ฺฉููู ุฏุฑ ูุชู  \n",
    "- **Word Embeddings (ูุซู Word2Vec ุง BERT)** โ ุชุจุฏู ฺฉููุงุช ุจู ุจุฑุฏุงุฑ ุนุฏุฏ  \n",
    "\n",
    "๐ **ุงฺฏุฑ ุงุฒ BERT ุงุณุชูุงุฏู ฺฉู:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "๐ **ููุจุน ูพุดููุงุฏ:**  \n",
    "[ููุฏููโุง ุจุฑ Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ๐ **ูุฑุญูู ถ: ูุฏูโุณุงุฒ (Training a Model)** (ูุซู ุณุงุฎุชู ุดุฎุตุช ุจุงุฒ)  \n",
    "ุจุงุฏ ุชุตูู ุจฺฏุฑ ฺฉู ุงุฒ ฺู ูุฏู ุงุณุชูุงุฏู ฺฉู:  \n",
    "โ **ูุฏูโูุง ุณุงุฏู (ูุซู Naive Bayes, SVM)** โ ุงฺฏุฑ ุฏุชุงุณุช ฺฉูฺฺฉ ุจุงุดู  \n",
    "โ **ูุฏูโูุง ูพฺุฏูโุชุฑ (ูุซู LSTM, BERT)** โ ุงฺฏุฑ ุฏุชุงุณุช ุจุฒุฑฺฏ ุจุงุดู ู ุฏูุช ุจุงูุง ุจุฎูุง  \n",
    "\n",
    "๐ **ุงฺฏุฑ ุงุฒ ูุฏู ุณุงุฏู ุงุณุชูุงุฏู ฺฉู:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "๐ **ุงฺฏุฑ ุงุฒ BERT ุงุณุชูุงุฏู ฺฉู:**  \n",
    "[ุฑุงูููุง ุขููุฒุด ูุฏู BERT ุจุฑุง ุชุญูู ุงุญุณุงุณุงุช](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## โ **ูุฑุญูู ท: ุงุฑุฒุงุจ ูุฏู (Evaluation & Testing)** (ูุซู ุจุฑุฑุณ ุงูุชุงุฒ ุจุงุฒ)  \n",
    "ุจุนุฏ ุงุฒ ุขููุฒุด ูุฏูุ ุจุงุฏ ุจุจู ฺูุฏุฑ ุฎูุจ ฺฉุงุฑ ูฺฉูู. ุงุฒ ูุนุงุฑูุง **ุฏูุช (Accuracy)ุ ุฏูุช ูุซุจุช (Precision)ุ ุงุฏุขูุฑ (Recall)** ุงุณุชูุงุฏู ฺฉู.  \n",
    "\n",
    "๐ **ฺฉุฏ ุงุฑุฒุงุจ ูุฏู:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "๐ **ููุจุน ูพุดููุงุฏ:**  \n",
    "[ุฑุงูููุง ุงุฑุฒุงุจ ูุฏูโูุง NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **ุญุงูุง ุชู ุจฺฏูุ ุชู ฺฉุฏูู ูุฑุญูู ุณูุงู ุฏุงุฑุ** ๐"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
