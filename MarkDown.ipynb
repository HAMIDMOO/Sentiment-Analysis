{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حله، حالا که دیتا رو پیدا کردی، بریم مرحله به مرحله ببینیم چیکار باید بکنی. جوری توضیح میدم که انگار داری یه بازی مرحله‌ای انجام میدی و هر مرحله رو باید درست رد کنی تا بری بعدی.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **مرحله ۱: بررسی اولیه دیتاست** (مثل نگاه کردن به نقشه قبل از شروع بازی)  \n",
    "قبل از اینکه هر کاری بکنی، باید یه نگاه کلی به دیتاستت بندازی. این کار کمک می‌کنه که بفهمی:  \n",
    "- **چند تا داده داری؟** (خب گفتی ۱۳,۰۰۰ تا، اوکی)  \n",
    "- **ستون‌های دیتاست چی هستن؟** (مثلاً، ستون متن توییت، ستون برچسب احساسات و ...)  \n",
    "- **مقدار داده‌ها در هر کلاس احساسات چطوره؟** (مثلاً، نکنه ۹۰٪ توییت‌ها \"خوشحال\" باشن و فقط ۱۰٪ بقیه؟ این باعث عدم تعادل میشه)  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی اولیه:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # اینو با اسم فایلت عوض کن\n",
    "print(df.head())  # نمایش ۵ سطر اول\n",
    "print(df.info())  # نمایش اطلاعات کلی درباره دیتاست\n",
    "print(df['label'].value_counts())  # بررسی توزیع برچسب‌ها\n",
    "```\n",
    "✍ **اگر توزیع داده‌ها نامتعادل بود، باید روش‌هایی مثل oversampling یا undersampling رو انجام بدی (بعداً بهش می‌رسیم).**  \n",
    "\n",
    "📌 **منبع پیشنهادی برای بررسی اولیه دیتاست:**  \n",
    "[راهنمای بررسی اولیه دیتا در پانداس](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 **مرحله ۲: پاکسازی داده‌ها (Data Cleaning)** (مثل تمیز کردن صفحه بازی قبل از شروع)  \n",
    "باید مطمئن بشی که دیتای خامت مشکل خاصی نداره. مشکلات رایج اینا هستن:  \n",
    "✅ **مقادیر خالی (Missing Values)**  \n",
    "✅ **داده‌های تکراری (Duplicate Data)**  \n",
    "✅ **توییت‌های بی‌معنی (مثلاً شامل فقط @ و # و لینک‌ها بدون متن مفید)**  \n",
    "\n",
    "📌 **کد پیشنهادی برای پاکسازی:**  \n",
    "```python\n",
    "# حذف مقادیر خالی\n",
    "df = df.dropna()\n",
    "\n",
    "# حذف مقادیر تکراری\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# بررسی نمونه‌هایی که فقط شامل لینک و منشن هستن\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[آموزش Data Cleaning در پانداس](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 **مرحله ۳: پیش‌پردازش متن (Text Preprocessing)** (مثل تنظیمات اولیه کاراکتر توی بازی)  \n",
    "الان باید متن‌های توییت رو برای مدل آماده کنیم. توییت‌ها معمولاً کثیف هستن و پر از چیزهای اضافی مثل:  \n",
    "❌ لینک‌ها (https://...)  \n",
    "❌ نام‌های کاربری (@user)  \n",
    "❌ هشتگ‌ها (#موضوع)  \n",
    "❌ ایموجی‌ها (😂😡❤️)  \n",
    "❌ حروف اضافی کشیده (\"عاااالیییی\")  \n",
    "\n",
    "📌 **کد پیشنهادی برای تمیز کردن متن:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # حذف لینک‌ها\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # حذف منشن‌ها\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # حذف هشتگ‌ها\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # حذف ی\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # حذف فاصله‌های اضافی\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای پیش‌پردازش متن](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **مرحله ۴: تحلیل داده‌ها (Exploratory Data Analysis - EDA)** (مثل بررسی ویژگی‌های بازی قبل از شروع)  \n",
    "باید یه کم روی دیتات آنالیز انجام بدی که بفهمی:  \n",
    "- **کدوم کلمات بیشتر استفاده شدن؟**  \n",
    "- **طول جملات چقدره؟**  \n",
    "- **احساسات چقدر توزیع شدن؟**  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی کلمات پر تکرار:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[EDA در NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 **مرحله ۵: تبدیل متن به عدد (Tokenization & Embedding)** (مثل آماده کردن کاراکترهای بازی برای حرکت)  \n",
    "مدل‌های یادگیری ماشین فقط عدد می‌فهمن، پس باید متن رو به عدد تبدیل کنیم. روش‌های مختلفی وجود داره:  \n",
    "- **Bag of Words (BoW)** – مدل ساده‌ای که فقط تعداد کلمات رو می‌شمره  \n",
    "- **TF-IDF** – مقدار اهمیت هر کلمه در متن  \n",
    "- **Word Embeddings (مثل Word2Vec یا BERT)** – تبدیل کلمات به بردار عددی  \n",
    "\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[مقدمه‌ای بر Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏗 **مرحله ۶: مدل‌سازی (Training a Model)** (مثل ساختن شخصیت بازی)  \n",
    "باید تصمیم بگیری که از چه مدلی استفاده کنی:  \n",
    "✅ **مدل‌های ساده (مثل Naive Bayes, SVM)** – اگر دیتاست کوچیک باشه  \n",
    "✅ **مدل‌های پیچیده‌تر (مثل LSTM, BERT)** – اگر دیتاست بزرگ باشه و دقت بالا بخوای  \n",
    "\n",
    "📌 **اگر از مدل ساده استفاده کنی:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "[راهنمای آموزش مدل BERT برای تحلیل احساسات](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **مرحله ۷: ارزیابی مدل (Evaluation & Testing)** (مثل بررسی امتیاز بازی)  \n",
    "بعد از آموزش مدل، باید ببینی چقدر خوب کار میکنه. از معیارهای **دقت (Accuracy)، دقت مثبت (Precision)، یادآوری (Recall)** استفاده کن.  \n",
    "\n",
    "📌 **کد ارزیابی مدل:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای ارزیابی مدل‌های NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **حالا تو بگو، تو کدوم مرحله سوال داری؟** 😎"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
