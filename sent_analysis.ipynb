{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Twitter Dataset\n",
    "About Dataset\n",
    "\n",
    "This dataset contains more than 3300 Persian tweets, crawled from Twitter.com\n",
    "Each tweet is assigned a label, which is a number between 0 to 4.\n",
    "Label 0 indicates the sentiment of Happiness and Joy.\n",
    "Label 1 indicates the sentiment of Sadness.\n",
    "Label 2 indicates the sentiment of Anger and Furiosity.\n",
    "Label 3 indicates the sentiment of Neutral.\n",
    "And finally, label 4 indicates the sentiment of intense emotions, such as Surprise, Fear, and Love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Numeric Labels</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù… ØºØ§ÛŒØª Ø²Ù†Ø¯Ú¯ÛŒ Ù…Ù† Ø§ÛŒÙ†Ù‡ Ú©Ù‡ ÛŒÚ©ÛŒ ÛŒØ±ÙˆØ² Ø¨ÙÙ‡Ù…Ù‡ Ø¯Ø§Ø±Ù… Ø±Ø§Ø¬Ø¹ Ø¨Ù‡ Ú†ÛŒ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†Ù…</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ø§ÛŒ Ø¨Ø§Ø¨Ø§ Ù†ØµÛŒØ¨ Ù‡Ø±Ú©Ø³ÛŒ Ù†Ù…ÛŒØ´Ù‡ Ú©Ù‡ØŒ ÙÙ‚Ø· Ø¯Ø¹Ø§ÛŒ Ø®ÛŒØ± Ù¾Ø¯Ø± Ù…Ø§Ø¯Ø± :))))))))</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ø¨Ø§ Ù…Ù† Ù…Ø«Ù„ Ø¨Ù‚ÛŒÙ‡ Ø±ÙØªØ§Ø± Ù†Ú©Ù† Ø¢Ø´ØºØ§Ù„</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ù…Ù† Ø¹Ù„Ø§Ù‚Ù‡ Ø´Ø¯ÛŒØ¯ÛŒ Ø¨Ù‡ Ø´Ù†ÛŒØ¯Ù† Â«Ø¨ØºÙ„Ù… Ú©Ù†Â» Ø§Ø² Ø·Ø±Ù Ø§ÙˆÙ†ÛŒ Ú©Ù‡ Â«Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨ØºÙ„Ø´ Ú©Ù†Ù…Â» Ø¯Ø§Ø±Ù….</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ú†ÛŒØ² Ú©ÛŒÚ© Ú¯Ø±Ù… Ùˆ Ù†Ø±Ù… Ù…ÛŒØ®ÙˆØ§Ù….</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>Ù…Ù…Ù„Ú©Øª Ù†ÛŒØ³Øª Ú©Ù‡ ØªÙˆÙ†Ù„ ÙˆØ­Ø´ØªÙ‡. Ø¨Ú†Ù‡ Ø±Ùˆ Ù…ÛŒØ¨Ø±Ù† Ù‚Ø§ØªÙ„ Ø¨Ø§Ø¨Ø§Ø´Ùˆ Ø¨Ø¨ÛŒÙ†Ù‡ØŒ Ø§ÙˆÙ† ÛŒÚ©ÛŒ Ø³Ø± Ø²Ù†Ø´Ùˆ Ù…ÛŒØ¨Ø±Ù‡. Ø¨Ø§Ø¨Ø§ Ù‚Ø±Ø§Ø±Ù‡ Ø¨Ø®ÙˆØ§Ø¨ÛŒÙ…Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ù….</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ Ø§Ù…ØªØ­Ø§Ù†Ø§ Ú©Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯ Ù…Ø«Ù„Ø§ ÛŒÚ©Ù… Ú©ØµÚ©Ù„Ú© Ú©Ù†Ù… Ú©Ù‡ Ù…Ø±ÛŒØ¶ÛŒ Ø²Ø¯ Ù‡Ù…Ø´Ùˆ Ù†Ø§Ø¨ÙˆØ¯ Ú©Ø±Ø¯. Ø§Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>Ø¯ÙˆØ³ØªØ§Ù† Ø§ÙˆÙ† Ø³Ù‡ Ù†Ù‚Ø·Ù‡ Ù‚Ø±Ù…Ø² Ù‡Ø§ Ù‡Ø³Øª Ú©Ù‡ Ù…ÛŒØ²Ù†ÛŒ Ø¨Ø§Ø² Ù…ÛŒØ´Ù‡ ØªØ§ Ù‡Ù…Ù‡ ÛŒ Ø§Ø³ØªØ§Ø¯Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒØ¯Ù† Ø±Ùˆ Ø¨Ø¨ÛŒÙ†ÛŒØ› Ù…Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ø­Ø¯ ÙÙ‡Ù…ÛŒØ¯Ù… Ø§ÙˆÙ†Ø§ Ú†ÛŒÙ‡. Ùˆ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ø­Ø¯Ù… Ù‡Ù… Ø±Ø§Ø¶ÛŒ Ø§Ù…. Ø´Ø¨ Ø®ÙˆØ´ğŸ–ï¸</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>Ù†ØªÛŒØ¬Ù‡ Ø±ÛŒØ³Ø±Ú† Ú©Ù…â€ŒÚ©Ù… Ø¯Ø§Ø±Ù‡ Ø¨Ù‡ Ø³Ù…ØªÛŒ Ø®ÙˆØ¨ÛŒ Ù¾ÛŒØ´ Ù…ÛŒØ±Ù‡</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>3380</td>\n",
       "      <td>Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡â€ŒÙ‡Ø§. Ú†Ù‡ Ø®Ø¨Ø±Ø§ØŸ</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3381 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  \\\n",
       "0              0   \n",
       "1              1   \n",
       "2              2   \n",
       "3              3   \n",
       "4              4   \n",
       "...          ...   \n",
       "3376        3376   \n",
       "3377        3377   \n",
       "3378        3378   \n",
       "3379        3379   \n",
       "3380        3380   \n",
       "\n",
       "                                                                                                                                                                        Tweets  \\\n",
       "0                                                                                                 Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù… ØºØ§ÛŒØª Ø²Ù†Ø¯Ú¯ÛŒ Ù…Ù† Ø§ÛŒÙ†Ù‡ Ú©Ù‡ ÛŒÚ©ÛŒ ÛŒØ±ÙˆØ² Ø¨ÙÙ‡Ù…Ù‡ Ø¯Ø§Ø±Ù… Ø±Ø§Ø¬Ø¹ Ø¨Ù‡ Ú†ÛŒ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†Ù…   \n",
       "1                                                                                                                 Ø§ÛŒ Ø¨Ø§Ø¨Ø§ Ù†ØµÛŒØ¨ Ù‡Ø±Ú©Ø³ÛŒ Ù†Ù…ÛŒØ´Ù‡ Ú©Ù‡ØŒ ÙÙ‚Ø· Ø¯Ø¹Ø§ÛŒ Ø®ÛŒØ± Ù¾Ø¯Ø± Ù…Ø§Ø¯Ø± :))))))))   \n",
       "2                                                                                                                                               Ø¨Ø§ Ù…Ù† Ù…Ø«Ù„ Ø¨Ù‚ÛŒÙ‡ Ø±ÙØªØ§Ø± Ù†Ú©Ù† Ø¢Ø´ØºØ§Ù„   \n",
       "3                                                                                                    Ù…Ù† Ø¹Ù„Ø§Ù‚Ù‡ Ø´Ø¯ÛŒØ¯ÛŒ Ø¨Ù‡ Ø´Ù†ÛŒØ¯Ù† Â«Ø¨ØºÙ„Ù… Ú©Ù†Â» Ø§Ø² Ø·Ø±Ù Ø§ÙˆÙ†ÛŒ Ú©Ù‡ Â«Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨ØºÙ„Ø´ Ú©Ù†Ù…Â» Ø¯Ø§Ø±Ù….   \n",
       "4                                                                                                                                                    Ú†ÛŒØ² Ú©ÛŒÚ© Ú¯Ø±Ù… Ùˆ Ù†Ø±Ù… Ù…ÛŒØ®ÙˆØ§Ù….   \n",
       "...                                                                                                                                                                        ...   \n",
       "3376                                                            Ù…Ù…Ù„Ú©Øª Ù†ÛŒØ³Øª Ú©Ù‡ ØªÙˆÙ†Ù„ ÙˆØ­Ø´ØªÙ‡. Ø¨Ú†Ù‡ Ø±Ùˆ Ù…ÛŒØ¨Ø±Ù† Ù‚Ø§ØªÙ„ Ø¨Ø§Ø¨Ø§Ø´Ùˆ Ø¨Ø¨ÛŒÙ†Ù‡ØŒ Ø§ÙˆÙ† ÛŒÚ©ÛŒ Ø³Ø± Ø²Ù†Ø´Ùˆ Ù…ÛŒØ¨Ø±Ù‡. Ø¨Ø§Ø¨Ø§ Ù‚Ø±Ø§Ø±Ù‡ Ø¨Ø®ÙˆØ§Ø¨ÛŒÙ…Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ù….   \n",
       "3377                                                                                       Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ Ø§Ù…ØªØ­Ø§Ù†Ø§ Ú©Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯ Ù…Ø«Ù„Ø§ ÛŒÚ©Ù… Ú©ØµÚ©Ù„Ú© Ú©Ù†Ù… Ú©Ù‡ Ù…Ø±ÛŒØ¶ÛŒ Ø²Ø¯ Ù‡Ù…Ø´Ùˆ Ù†Ø§Ø¨ÙˆØ¯ Ú©Ø±Ø¯. Ø§Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡   \n",
       "3378  Ø¯ÙˆØ³ØªØ§Ù† Ø§ÙˆÙ† Ø³Ù‡ Ù†Ù‚Ø·Ù‡ Ù‚Ø±Ù…Ø² Ù‡Ø§ Ù‡Ø³Øª Ú©Ù‡ Ù…ÛŒØ²Ù†ÛŒ Ø¨Ø§Ø² Ù…ÛŒØ´Ù‡ ØªØ§ Ù‡Ù…Ù‡ ÛŒ Ø§Ø³ØªØ§Ø¯Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒØ¯Ù† Ø±Ùˆ Ø¨Ø¨ÛŒÙ†ÛŒØ› Ù…Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ø­Ø¯ ÙÙ‡Ù…ÛŒØ¯Ù… Ø§ÙˆÙ†Ø§ Ú†ÛŒÙ‡. Ùˆ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ø­Ø¯Ù… Ù‡Ù… Ø±Ø§Ø¶ÛŒ Ø§Ù…. Ø´Ø¨ Ø®ÙˆØ´ğŸ–ï¸   \n",
       "3379                                                                                                                             Ù†ØªÛŒØ¬Ù‡ Ø±ÛŒØ³Ø±Ú† Ú©Ù…â€ŒÚ©Ù… Ø¯Ø§Ø±Ù‡ Ø¨Ù‡ Ø³Ù…ØªÛŒ Ø®ÙˆØ¨ÛŒ Ù¾ÛŒØ´ Ù…ÛŒØ±Ù‡    \n",
       "3380                                                                                                                                                     Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡â€ŒÙ‡Ø§. Ú†Ù‡ Ø®Ø¨Ø±Ø§ØŸ   \n",
       "\n",
       "      Numeric Labels             Label  \n",
       "0                  4  Intense Emotions  \n",
       "1                  0             Happy  \n",
       "2                  2             Angry  \n",
       "3                  4  Intense Emotions  \n",
       "4                  0             Happy  \n",
       "...              ...               ...  \n",
       "3376               2             Angry  \n",
       "3377               2             Angry  \n",
       "3378               0             Happy  \n",
       "3379               0             Happy  \n",
       "3380               3           Neutral  \n",
       "\n",
       "[3381 rows x 4 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2= pd.read_csv('./PersianTwitterDataset.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø­ÛŒÙ Ú©Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø§ÛŒÙ†Ø§ Ú©Ù†Ø§Ø± Ù‡Ù… Ù†ÛŒØ³ØªÙ† Ø§Ù†ØªÙ‚Ø§Ù… Ø§Ù…Ø´Ø¨ Ø±Ùˆ Ø¨Ú¯ÛŒØ±Ù†ØŒ Ù‡Ø±Ú†Ù†Ø¯ Ù‡Ø´ØªØ§ Ú¯Ù„ Ø¯ÛŒÚ¯Ù‡ ÙˆØ§Ù‚Ø¹Ø§ Ù‚ÙÙ„Ù‡ ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
      "Happy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 3300)\n",
    "print(df2.loc[i, 'Tweets'])\n",
    "print(df2.loc[i, 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian tweets emotional dataset\n",
    "About Dataset\n",
    "New Persian Dataset\n",
    "Since Persian datasets are really scarce I scrape Twitter in order to make a new Persian dataset.\n",
    "\n",
    "The tweets have been pulled from Twitter using snscrape and manual tagging has been done based on Ekman's 6 main emotions.\n",
    "For privacy sake, I pre-process and remove usernames, display names, and mentions from all tweets. Also, I deleted the timestamps and Tweets IDs.\n",
    "\n",
    "Columns:\n",
    "1) tweet\n",
    "2) replyCount\n",
    "3) retweetCount\n",
    "4) likeCount\n",
    "5) quoteCount\n",
    "6) hashtags\n",
    "7) sourceLabel\n",
    "8) emotion\n",
    "\n",
    "Please leave an upvote if you find this relevant. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "sad         34328\n",
       "joy         28024\n",
       "anger       20069\n",
       "fear        17624\n",
       "surprise    12859\n",
       "disgust       925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('./merged.csv')\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6156 entries, 0 to 6155\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   tweet    6151 non-null   object\n",
      " 1   emotion  6154 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 96.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_excel('./train_fa.xlsx')\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet      5\n",
       "emotion    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ù… Ø¯Ù…Ø§ÛŒ ØµØ¨Ø­ Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù…</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ú©ÛŒÙÛŒØªØ´ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ø¨Ø§ Ø´Ú© Ø®Ø±ÛŒØ¯Ù… ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒÙ… Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø¯ÙˆØ¯ 2 Ù…Ø§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡Â«Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¨Ø§Ø®Ø¨Ø± Ø´Ø¯ÛŒÙ…Â» Ú©Ù‡ ÙØ±Ø¯ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù†Ø´Ù‡Ø± Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ 41 Ø¯Ø®ØªØ± ØªØ¬Ø§ÙˆØ² Ú©Ø±Ø¯Ù‡. Ø§Ù…ÛŒØ¯ÙˆØ§Ø±ÛŒÙ… Ø¨Ø§ Ù‡Ù…Ú©Ø§Ø±ÛŒ Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯Ø§Ù† Ù…Ø­ØªØ±Ù… Ù…Ø¬Ù„Ø³ Ùˆ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù‚Ø¶Ø§ØŒ Ø¯ÛŒÚ¯Ù‡ Ø¨Ù‡ Ø§ÛŒÙ† Ø±Ø§Ø­ØªÛŒ Ø¨Ø§Ø®Ø¨Ø± Ù†Ø´ÛŒÙ….</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú†ÙˆÙ† Ù‡Ù…Ø´ Ø¨Ø§ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø«Ø¨Øª Ø´Ø¯Ù‡ ØŒ Ø§ÛŒØ§ Ù…ÛŒØ´Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶ Ø²Ø¯ØŸØŸ Ùˆ Ø§ØµÙ† ØªØ§Ø«ÛŒØ± Ø¯Ø§Ø±Ù‡ØŸ Ú©Ø³ÛŒ Ø§Ú¯Ù‡ Ø§Ø·Ù„Ø§Ø¹ÛŒ Ø¯Ø§Ø±Ù‡ Ù…Ù…Ù†ÙˆÙ† Ù…ÛŒØ´Ù… Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ú©Ù†ÛŒØ¯</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§ÙŠÙ† ÙˆØ¶Ø¹ Ø¨ Ø·Ø±Ø² Ø®Ù†Ø¯Ù‡ Ø¯Ø§Ø±ÙŠ Ú¯Ø±ÙŠÙ‡ Ø¯Ø§Ø±Ù‡ ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø®Ø¨ Ù…Ù† Ø±Ø³Ù…Ø§ Ø§Ø² ÛŒÚ© Ù†ÙØ± Ù…ØªÙ†ÙØ±Ù…ØŒÚ†ÙˆÙ† Ø§Ø² Ú¯Ø±Ø¨Ù‡ Ø¨Ø¯Ø´ Ù…ÛŒØ§Ø¯ Ø§Ø² ØµØ¨Ø­ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯Ù‡ Ø±Ùˆ Ù…Ø® Ù…Ù† Ø±Ø§Ù‡ Ø±ÙØªÙ†Ø›Ø´Ù¾Ø´ Ù…ÛŒÚ¯ÛŒØ±ÛŒØŒÚ©Ø²Ø§Ø² Ù…ÛŒÚ¯ÛŒØ±ÛŒØŒÚ©Ú© Ø¯Ø§Ø±Ù‡ ØŒØ¯Ø±Ø¯ Ø¯Ø§Ø±Ù‡ØŒÙ…Ø±Ø¶ Ø¯Ø§Ø±Ù‡ Ùˆ Ø§ÛŒÙ†Ù‚Ø¯Ø± ØªÚ©Ø±Ø§Ø± Ú©Ø±Ø¯Ù‡ Ú©Ù‡ ØªØ§ Ø³Ø±Ù… Ù…ÛŒØ®Ø§Ø±Ù‡ Ù…ÛŒÙ¾Ø±Ù… Ø¬Ù„ÙˆÛŒ Ø¢ÛŒÙ†Ù‡ Ù†Ú¯Ø§Ù‡ Ù…ÛŒÚ©Ù†Ù… ÛŒÙ‡ ÙˆÙ‚Øª Ø´Ù¾Ø´ Ù†Ú¯Ø±ÙØªÙ‡ Ø¨Ø§Ø´Ù…... Ø¹ÙˆØ¶ÛŒ Ø§Ø²Ø´ Ù…ØªÙ†ÙØ±Ù… Ø¨Ù‡Ø´ Ú¯ÙØªÙ‡ Ø¨ÙˆØ¯Ù… Ù…Ù†Ùˆ Ø±Ùˆ ÛŒÙ‡ Ú†ÛŒØ²ÛŒ Ø­Ø³Ø§Ø³ Ù†Ú©Ù†</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>Ù…Ø±Ø­ÙˆÙ… Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø¢Ø¨Ú©ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒÚ©Ø±Ø¯     Ù…Ø±Ø­ÙˆÙ… Ø¹Ø¬Ø¨ Ø¢ÛŒÙ†Ø¯Ù‡ Ù†Ú¯Ø±ÛŒ Ùˆ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø´Øª</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>Ú©Ù„Ø§ Ø¹ÛŒÙ† Ø§Ø¹ØªÙ‚Ø§Ø¯Ø§Øª Ùˆ ØªÙˆØ¦ÛŒØª Ø²Ø¯Ù†Ø§ØªÙˆÙ† ... !!   Ø¯Ø± Ù‚Ø¨Ø§Ù„ Ø±Ø§Ù†Øª Ùˆ Ù†ÙˆÙ† Ù…ÙØª Ø³ÙØ±Ù‡ Ø§Ù†Ù‚Ù„Ø§Ø¨ ...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>Ø®Ø¨ ÙˆÙ‚ØªÛŒ Ù…ÛŒÚ¯ÛŒ Ú©Ø³ÛŒ Ø¨ÛŒØ§Ø¯ Ù…Ø§Ø±Ùˆ Ø¨Ú¯ÛŒØ±Ù‡ ÛŒØ§Ø±Ùˆ ØªØ±Ø³ Ù…ÛŒÚ©Ù†Ù‡  ÛŒÚ©Ù… Ø¯ÙˆØ³Øª Ø¨Ø§Ø´ÛŒØ¯ ØªØ§ Ø§ÙˆÙ† Ø­Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ÙˆØ± Ø±Ùˆ Ø¨Ø¨ÛŒÙ†Ù‡ Ø¯Ø± Ø´Ù…Ø§.   Ø§ÙˆÙ†ÙˆÙ‚Øª Ø§ÙˆÙ† Ø·Ø±Ù Ú©Ù†Ù‡ Ù…ÛŒØ´Ù‡ ÙˆØ§Ø³Ù‡ Ø§Ø²Ø¯ÙˆØ§Ø¬</td>\n",
       "      <td>FEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>Ù‡Ù…ÙˆÙ† Ù‡Ø§Ø±Ùˆ     Ù…Ú¯Ù‡ Ø¢Ù‡Ù†Ú¯ Ø¬Ø¯ÛŒØ¯Ø§ÛŒ Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‡Ù‡ Ù¾Ù†Ø¬Ø§Ù‡ Ø±Ùˆ Ú¯ÙˆØ´ Ù…ÛŒØ¯ÛŒØ¯ ØŸ</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>Ù†ÛŒÙ… Ø¯Ú¯ÛŒØ±Ø´ Ú†Ø·ÙˆØ± Ø­Ù„ Ù†ÛŒØ´Ø¯</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6112 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                      tweet  \\\n",
       "0                                                                                                                                                                                                                                            Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ù… Ø¯Ù…Ø§ÛŒ ØµØ¨Ø­ Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù…   \n",
       "1                                                        \"Ú©ÛŒÙÛŒØªØ´ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ø¨Ø§ Ø´Ú© Ø®Ø±ÛŒØ¯Ù… ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒÙ… Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø¯ÙˆØ¯ 2 Ù…Ø§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡Â«Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¨Ø§Ø®Ø¨Ø± Ø´Ø¯ÛŒÙ…Â» Ú©Ù‡ ÙØ±Ø¯ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù†Ø´Ù‡Ø± Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ 41 Ø¯Ø®ØªØ± ØªØ¬Ø§ÙˆØ² Ú©Ø±Ø¯Ù‡. Ø§Ù…ÛŒØ¯ÙˆØ§Ø±ÛŒÙ… Ø¨Ø§ Ù‡Ù…Ú©Ø§Ø±ÛŒ Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯Ø§Ù† Ù…Ø­ØªØ±Ù… Ù…Ø¬Ù„Ø³ Ùˆ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù‚Ø¶Ø§ØŒ Ø¯ÛŒÚ¯Ù‡ Ø¨Ù‡ Ø§ÛŒÙ† Ø±Ø§Ø­ØªÛŒ Ø¨Ø§Ø®Ø¨Ø± Ù†Ø´ÛŒÙ….   \n",
       "2                                                                                                                                                                           Ú†ÙˆÙ† Ù‡Ù…Ø´ Ø¨Ø§ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø«Ø¨Øª Ø´Ø¯Ù‡ ØŒ Ø§ÛŒØ§ Ù…ÛŒØ´Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶ Ø²Ø¯ØŸØŸ Ùˆ Ø§ØµÙ† ØªØ§Ø«ÛŒØ± Ø¯Ø§Ø±Ù‡ØŸ Ú©Ø³ÛŒ Ø§Ú¯Ù‡ Ø§Ø·Ù„Ø§Ø¹ÛŒ Ø¯Ø§Ø±Ù‡ Ù…Ù…Ù†ÙˆÙ† Ù…ÛŒØ´Ù… Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ú©Ù†ÛŒØ¯   \n",
       "3                                                                                                                                                                                                                                                     Ø§ÙŠÙ† ÙˆØ¶Ø¹ Ø¨ Ø·Ø±Ø² Ø®Ù†Ø¯Ù‡ Ø¯Ø§Ø±ÙŠ Ú¯Ø±ÙŠÙ‡ Ø¯Ø§Ø±Ù‡ ...   \n",
       "4     Ø®Ø¨ Ù…Ù† Ø±Ø³Ù…Ø§ Ø§Ø² ÛŒÚ© Ù†ÙØ± Ù…ØªÙ†ÙØ±Ù…ØŒÚ†ÙˆÙ† Ø§Ø² Ú¯Ø±Ø¨Ù‡ Ø¨Ø¯Ø´ Ù…ÛŒØ§Ø¯ Ø§Ø² ØµØ¨Ø­ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯Ù‡ Ø±Ùˆ Ù…Ø® Ù…Ù† Ø±Ø§Ù‡ Ø±ÙØªÙ†Ø›Ø´Ù¾Ø´ Ù…ÛŒÚ¯ÛŒØ±ÛŒØŒÚ©Ø²Ø§Ø² Ù…ÛŒÚ¯ÛŒØ±ÛŒØŒÚ©Ú© Ø¯Ø§Ø±Ù‡ ØŒØ¯Ø±Ø¯ Ø¯Ø§Ø±Ù‡ØŒÙ…Ø±Ø¶ Ø¯Ø§Ø±Ù‡ Ùˆ Ø§ÛŒÙ†Ù‚Ø¯Ø± ØªÚ©Ø±Ø§Ø± Ú©Ø±Ø¯Ù‡ Ú©Ù‡ ØªØ§ Ø³Ø±Ù… Ù…ÛŒØ®Ø§Ø±Ù‡ Ù…ÛŒÙ¾Ø±Ù… Ø¬Ù„ÙˆÛŒ Ø¢ÛŒÙ†Ù‡ Ù†Ú¯Ø§Ù‡ Ù…ÛŒÚ©Ù†Ù… ÛŒÙ‡ ÙˆÙ‚Øª Ø´Ù¾Ø´ Ù†Ú¯Ø±ÙØªÙ‡ Ø¨Ø§Ø´Ù…... Ø¹ÙˆØ¶ÛŒ Ø§Ø²Ø´ Ù…ØªÙ†ÙØ±Ù… Ø¨Ù‡Ø´ Ú¯ÙØªÙ‡ Ø¨ÙˆØ¯Ù… Ù…Ù†Ùˆ Ø±Ùˆ ÛŒÙ‡ Ú†ÛŒØ²ÛŒ Ø­Ø³Ø§Ø³ Ù†Ú©Ù†   \n",
       "...                                                                                                                                                                                                                                                                                     ...   \n",
       "6151                                                                                                                                                                                                         Ù…Ø±Ø­ÙˆÙ… Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø¢Ø¨Ú©ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒÚ©Ø±Ø¯     Ù…Ø±Ø­ÙˆÙ… Ø¹Ø¬Ø¨ Ø¢ÛŒÙ†Ø¯Ù‡ Ù†Ú¯Ø±ÛŒ Ùˆ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø´Øª     \n",
       "6152                                                                                                                                                                                                       Ú©Ù„Ø§ Ø¹ÛŒÙ† Ø§Ø¹ØªÙ‚Ø§Ø¯Ø§Øª Ùˆ ØªÙˆØ¦ÛŒØª Ø²Ø¯Ù†Ø§ØªÙˆÙ† ... !!   Ø¯Ø± Ù‚Ø¨Ø§Ù„ Ø±Ø§Ù†Øª Ùˆ Ù†ÙˆÙ† Ù…ÙØª Ø³ÙØ±Ù‡ Ø§Ù†Ù‚Ù„Ø§Ø¨ ...   \n",
       "6153                                                                                                                                          Ø®Ø¨ ÙˆÙ‚ØªÛŒ Ù…ÛŒÚ¯ÛŒ Ú©Ø³ÛŒ Ø¨ÛŒØ§Ø¯ Ù…Ø§Ø±Ùˆ Ø¨Ú¯ÛŒØ±Ù‡ ÛŒØ§Ø±Ùˆ ØªØ±Ø³ Ù…ÛŒÚ©Ù†Ù‡  ÛŒÚ©Ù… Ø¯ÙˆØ³Øª Ø¨Ø§Ø´ÛŒØ¯ ØªØ§ Ø§ÙˆÙ† Ø­Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ÙˆØ± Ø±Ùˆ Ø¨Ø¨ÛŒÙ†Ù‡ Ø¯Ø± Ø´Ù…Ø§.   Ø§ÙˆÙ†ÙˆÙ‚Øª Ø§ÙˆÙ† Ø·Ø±Ù Ú©Ù†Ù‡ Ù…ÛŒØ´Ù‡ ÙˆØ§Ø³Ù‡ Ø§Ø²Ø¯ÙˆØ§Ø¬    \n",
       "6154                                                                                                                                                                                                                     Ù‡Ù…ÙˆÙ† Ù‡Ø§Ø±Ùˆ     Ù…Ú¯Ù‡ Ø¢Ù‡Ù†Ú¯ Ø¬Ø¯ÛŒØ¯Ø§ÛŒ Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‡Ù‡ Ù¾Ù†Ø¬Ø§Ù‡ Ø±Ùˆ Ú¯ÙˆØ´ Ù…ÛŒØ¯ÛŒØ¯ ØŸ   \n",
       "6155                                                                                                                                                                                                                                                                Ù†ÛŒÙ… Ø¯Ú¯ÛŒØ±Ø´ Ú†Ø·ÙˆØ± Ø­Ù„ Ù†ÛŒØ´Ø¯    \n",
       "\n",
       "       emotion  \n",
       "0         HATE  \n",
       "1          SAD  \n",
       "2        OTHER  \n",
       "3          SAD  \n",
       "4         HATE  \n",
       "...        ...  \n",
       "6151  SURPRISE  \n",
       "6152     ANGRY  \n",
       "6153      FEAR  \n",
       "6154  SURPRISE  \n",
       "6155     OTHER  \n",
       "\n",
       "[6112 rows x 2 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# drop the rows tha have nan values\n",
    "df3= df3.dropna()\n",
    "print(df3.isna().sum())\n",
    "\n",
    "# drop the duplicated values\n",
    "df3= df3.drop_duplicates()\n",
    "print(df3.duplicated().sum())\n",
    "df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # Ø­Ø°Ù Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§ (@mentions)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # Ø­Ø°Ù Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ (#hashtags)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "    return text\n",
    "\n",
    "df3['clean_text'] = df3['tweet'].apply(clean_text)\n",
    "\n",
    "\n",
    "# i = df3.apply(lambda row: 1 if row['clean_text'] != row['tweet'] else 0, axis=1).sum()\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_to_remove = df3[df3['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "# # Ú†Ø§Ù¾ Ø³Ø·Ø±â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯\n",
    "# print(\"Ø³Ø·Ø±â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡:\")\n",
    "# print(len(rows_to_remove))\n",
    "\n",
    "\n",
    "# # Filter rows that only contain links, @ or #\n",
    "# df3 = df3[~df3['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Ø¯Ùˆ ØµØ¨Ø­ Ù…ØªÙˆØ§Ù„ÛŒ Ø±Ø§Ù†Ù†Ø¯Ù‡ ØªØ§Ú©Ø³ÛŒ Ø§Ø¹ØµØ§Ø¨Ù… Ø¬ÙˆÛŒÛŒØ¯ Ú©Ù‡ Ú†Ø±Ø§ Ù¾Ù†Ø¬ ØªÙˆÙ…Ø§Ù†ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ùˆ Ø®Ø±Ø¯ Ù†Ø¯Ø§Ø±ÛŒØ¯ Ø§ÛŒÙ† ØªØ®Ù„ÛŒÙ‡ Ùˆ ØºØ± Ø²Ø¯Ù† Ø±ÙˆØ²ÛŒ ØªÙ…Ø§Ù…ÛŒ Ø¯Ø§Ø±Ø¯\n",
      "Processed Text: ['ØµØ¨Ø­', 'Ù…ØªÙˆØ§Ù„ÛŒ', 'Ø±Ø§Ù†Ù†Ø¯Ù‡', 'ØªØ§Ú©Ø³ÛŒ', 'Ø§Ø¹ØµØ§Ø¨', 'Ø¬ÙˆÛŒÛŒØ¯', 'ØªÙˆÙ…Ø§Ù†ÛŒ', 'Ø¯Ø§Ø±ÛŒØ¯', 'Ø®Ø±Ø¯', 'Ù†Ø¯Ø§Ø±ÛŒØ¯', 'ØªØ®Ù„ÛŒÙ‡', 'ØºØ±', 'Ø²Ø¯Ù†', 'Ø±ÙˆØ²']\n"
     ]
    }
   ],
   "source": [
    "from hazm import Normalizer, WordTokenizer, stopwords_list, Lemmatizer\n",
    "import random\n",
    "import string\n",
    "\n",
    "def preprocessing(text, Apply_normalizer= True, Aplly_wordtokenizer= True, remove_stop_words= True, Apply_lemmatizer=True, remove_numbers=True):\n",
    "  '''\n",
    "  Cleaning and preprocessing the given text.\n",
    "\n",
    "  Args:\n",
    "    text (str): the input text that we want to work on in.\n",
    "    Apply_normalizer (bool): if True, shows that text will bee normalize by hazm. Defualt is True\n",
    "    Aplly_wordtokenizer (bool): if True, the text will tokenize. Defualt is True\n",
    "    remove_stop_words (list): a list that have the set of stopwords in pesian. Defualt is True\n",
    "    Apply_lemmatizer (bool): change the word to its root. Defualt is True\n",
    "    remove_numbers (bool): remove the numbers from text\n",
    "    replace the \\u200c:( text = \"Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ù… Ú©ØªØ§Ø¨ÛŒ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡ Ùˆ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù….\"\n",
    "                        processed_text = ['Ù…ÛŒ\\u200cØ®ÙˆØ§Ù†Ù…', 'Ú©ØªØ§Ø¨ÛŒ', 'Ø§Ø²', 'Ú©ØªØ§Ø¨\\u200cØ®Ø§Ù†Ù‡', 'Ùˆ', 'Ø¯Ø§Ù†Ø´\\u200cØ¢Ù…ÙˆØ²Ø§Ù†', 'Ø±Ø§', 'Ù…ÛŒ\\u200cØ¨ÛŒÙ†Ù…'])\n",
    "    string.punctuation : removes this karakters (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
    "\n",
    "  Return:\n",
    "    filtered (list): list of words(of text) after cleaning and preprocessing.\n",
    "  '''\n",
    "\n",
    "  \n",
    "  # Initinalize requiared tools from Hazm\n",
    "  normalizer= Normalizer()\n",
    "  wordtokenizer= WordTokenizer()\n",
    "  stopword_lst= stopwords_list()\n",
    "  lemmatizer = Lemmatizer()\n",
    "  panctuation= string.punctuation +  \"ØŸØŒØ›.,#\"\n",
    "  \n",
    "\n",
    "\n",
    "  # Step 1 : Normalize the text\n",
    "  if Apply_normalizer:\n",
    "    text= normalizer.normalize(text)\n",
    "\n",
    "\n",
    "    \n",
    "  # Step 2: Tokenize the text to words\n",
    "  if Aplly_wordtokenizer:\n",
    "    words= wordtokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "  # Step 3: delete the aditional numbers and words\n",
    "  if remove_numbers:\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "\n",
    "\n",
    "  # step 4 : Delet the stop words from  words(list)\n",
    "  if remove_stop_words:\n",
    "    words= [word for word in words if word not in stopword_lst]\n",
    "\n",
    "\n",
    "\n",
    "  # Step 5: delete the additionala (.?!,)\n",
    "  if panctuation:\n",
    "    words= [word for word in words if word not in panctuation]  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  #step 6: find the root of words\n",
    "  if Apply_lemmatizer:\n",
    "    words= [lemmatizer.lemmatize(word, pos= 'v') for word in words]\n",
    "\n",
    "\n",
    "  #ÙStep 7: replace the '\\u200c'  \n",
    "  filtered= [word.replace('\\u200c', '')for word in words]\n",
    "\n",
    "  return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Kodun ne yaptÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in ğŸ˜Š\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x= random.randint(1, 6000)\n",
    "    sample_text = df3['clean_text'][x]\n",
    "    processed_text = preprocessing(\n",
    "        sample_text)\n",
    "    print(\"Original Text:\", sample_text)\n",
    "    print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3[\"clean_text\"] = df3[\"clean_text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¹Ú©Ø³ÛŒ Ú©Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ² Ù¾ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø§Ù„Ø§Ù†Ø´Ù‡... ÙˆØ³Ø· Ú©ÙˆÙ‡ Ù‡Ø§... Ù„Ø§Ø¨Ù‡ Ù„Ø§ÛŒ Ù…Ù‡... #ØªØ¨Ø±ÛŒØ²</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ØªÙ†Ù‡Ø§ÛŒÛŒÙ… Ø´Ø¨ÛŒÙ‡ ØªÙ†Ù‡Ø§ÛŒÛŒÙ‡ Ø¸Ù‡Ø±Ø§ÛŒ Ø¨Ú†Ú¯ÛŒÙ… Ø´Ø¯Ù‡ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…ÙŠ Ø®ÙˆØ§Ø¨ÙŠØ¯Ù† Ùˆ Ù…Ù† Ø®ÙˆØ§Ø¨Ù… Ù†Ù…ÙŠ Ø¨Ø±Ø¯ØŒ Ø¢Ø¯Ù…Ø§ Ø¨ÙˆØ¯Ù† Ø§Ù…Ø§ Ù†Ø¨ÙˆØ¯Ù†</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø®ÙˆØ¨Ù‡ ØªÙ…Ø§Ù… Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙˆØ´ÛŒ Ø±Ùˆ Ù…Ø­Ø§ÙØ¸Øª Ù…ÛŒâ€ŒÚ©Ù†Ù‡</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§ÛŒÙ† Ø®Ø§Ú© Ù…Ø§Ù„ Ù…Ø±Ø¯Ù…Ø§Ù† Ø§Ø³Øª Ù†Ù‡ Ø­Ø§Ú©Ù…Ø§Ù† #Ø§ÛŒØ±Ø§Ù† #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§Ú¯Ù‡ ØªÙˆ Ø¨ØºÙ„Øª Ø¨ÙˆØ¯Ù… Ø­Ø§Ù„Ù… Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒØ´Ø¯</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>Ù…Ù† Ø§Ø² Ø¨Ùˆ Ùˆ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±ÛŒØ´ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… ØŒ Ù‚ÛŒÙ…ØªØ´ Ù‡Ù…â€Œ Ù…Ù†Ø§Ø³Ø¨Ù‡</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>Ú¯Ø§Ø² Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¢Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø±Ù‚ Ù†Ø¯Ø§Ø±ÛŒÙ… Ù†Øª Ù†Ø¯Ø§Ø±ÛŒÙ… Ù¾ÙˆÙ„ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø§Ø¹ØµØ§Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø²Ù†Ø¯Ú¯ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ… ÛŒÙ‡ Ø¯ÙˆÙ†Ù‡ Ø¯Ø§Ø±ÛŒÙ… ÙØ¹Ù„Ø§</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>ÛŒÚ©ÛŒ Ø¨Ù‡Ù… Ú¯ÙØª Ø¨Ø±Ù†Ùˆ Ú†Ø±Ø§ Ø¹Ø§Ø´Ù‚ Ù†Ù…ÛŒØ´ÛŒ Ú¯ÙØªÙ… Ù…Ø§ Ù¾ÙˆÙ„ Ø¹Ø§Ø´Ù‚ÛŒ Ú©Ø±Ø¯Ù†Ùˆ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ÛŒØ¯ Ø¨Ø´ÛŒÙ†ÛŒÙ… Ø¹Ø§Ø´Ù‚Ø§Ø±Ùˆ Ù†Ú¯Ø§Ù‡ Ú©Ù†ÛŒÙ… #ØªÙ„Ø®</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ù‚Ø¶ÛŒÙ‡ ÛŒ Ú¯Ø§Ø² Ù…ÛŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ… ÙÙ‚Ø· ÙØ±Ø§Ø®ÙˆØ§Ù† Û²Û¹ Ùˆ Û³Û° #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>Ø³Ù„Ø§Ù…. Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ§Ø¸Ø¨Øª Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ÙˆÛŒØ±ÙˆØ³ Ú©ÙˆÙØªÛŒ Ø±Ùˆâ€Œ Ù†Ú¯ÛŒØ±ÛŒØ¯ ?? Ù¾Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ù…Ø§ Ø±Ùˆ</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4923 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†  \\\n",
       "0                 Ø¹Ú©Ø³ÛŒ Ú©Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ² Ù¾ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø§Ù„Ø§Ù†Ø´Ù‡... ÙˆØ³Ø· Ú©ÙˆÙ‡ Ù‡Ø§... Ù„Ø§Ø¨Ù‡ Ù„Ø§ÛŒ Ù…Ù‡... #ØªØ¨Ø±ÛŒØ²    \n",
       "1     ØªÙ†Ù‡Ø§ÛŒÛŒÙ… Ø´Ø¨ÛŒÙ‡ ØªÙ†Ù‡Ø§ÛŒÛŒÙ‡ Ø¸Ù‡Ø±Ø§ÛŒ Ø¨Ú†Ú¯ÛŒÙ… Ø´Ø¯Ù‡ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…ÙŠ Ø®ÙˆØ§Ø¨ÙŠØ¯Ù† Ùˆ Ù…Ù† Ø®ÙˆØ§Ø¨Ù… Ù†Ù…ÙŠ Ø¨Ø±Ø¯ØŒ Ø¢Ø¯Ù…Ø§ Ø¨ÙˆØ¯Ù† Ø§Ù…Ø§ Ù†Ø¨ÙˆØ¯Ù†   \n",
       "2                                                                Ø®ÙˆØ¨Ù‡ ØªÙ…Ø§Ù… Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙˆØ´ÛŒ Ø±Ùˆ Ù…Ø­Ø§ÙØ¸Øª Ù…ÛŒâ€ŒÚ©Ù†Ù‡   \n",
       "3                                                     Ø§ÛŒÙ† Ø®Ø§Ú© Ù…Ø§Ù„ Ù…Ø±Ø¯Ù…Ø§Ù† Ø§Ø³Øª Ù†Ù‡ Ø­Ø§Ú©Ù…Ø§Ù† #Ø§ÛŒØ±Ø§Ù† #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ   \n",
       "4                                                                    Ø§Ú¯Ù‡ ØªÙˆ Ø¨ØºÙ„Øª Ø¨ÙˆØ¯Ù… Ø­Ø§Ù„Ù… Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒØ´Ø¯   \n",
       "...                                                                                                   ...   \n",
       "4918                                                    Ù…Ù† Ø§Ø² Ø¨Ùˆ Ùˆ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±ÛŒØ´ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… ØŒ Ù‚ÛŒÙ…ØªØ´ Ù‡Ù…â€Œ Ù…Ù†Ø§Ø³Ø¨Ù‡   \n",
       "4919    Ú¯Ø§Ø² Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¢Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø±Ù‚ Ù†Ø¯Ø§Ø±ÛŒÙ… Ù†Øª Ù†Ø¯Ø§Ø±ÛŒÙ… Ù¾ÙˆÙ„ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø§Ø¹ØµØ§Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø²Ù†Ø¯Ú¯ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ… ÛŒÙ‡ Ø¯ÙˆÙ†Ù‡ Ø¯Ø§Ø±ÛŒÙ… ÙØ¹Ù„Ø§   \n",
       "4920    ÛŒÚ©ÛŒ Ø¨Ù‡Ù… Ú¯ÙØª Ø¨Ø±Ù†Ùˆ Ú†Ø±Ø§ Ø¹Ø§Ø´Ù‚ Ù†Ù…ÛŒØ´ÛŒ Ú¯ÙØªÙ… Ù…Ø§ Ù¾ÙˆÙ„ Ø¹Ø§Ø´Ù‚ÛŒ Ú©Ø±Ø¯Ù†Ùˆ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ÛŒØ¯ Ø¨Ø´ÛŒÙ†ÛŒÙ… Ø¹Ø§Ø´Ù‚Ø§Ø±Ùˆ Ù†Ú¯Ø§Ù‡ Ú©Ù†ÛŒÙ… #ØªÙ„Ø®   \n",
       "4921                                  Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ù‚Ø¶ÛŒÙ‡ ÛŒ Ú¯Ø§Ø² Ù…ÛŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ… ÙÙ‚Ø· ÙØ±Ø§Ø®ÙˆØ§Ù† Û²Û¹ Ùˆ Û³Û° #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ   \n",
       "4922                                  Ø³Ù„Ø§Ù…. Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ§Ø¸Ø¨Øª Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ÙˆÛŒØ±ÙˆØ³ Ú©ÙˆÙØªÛŒ Ø±Ùˆâ€Œ Ù†Ú¯ÛŒØ±ÛŒØ¯ ?? Ù¾Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ù…Ø§ Ø±Ùˆ   \n",
       "\n",
       "        SAD  \n",
       "0     OTHER  \n",
       "1       SAD  \n",
       "2     HAPPY  \n",
       "3     ANGRY  \n",
       "4       SAD  \n",
       "...     ...  \n",
       "4918  HAPPY  \n",
       "4919    SAD  \n",
       "4920    SAD  \n",
       "4921  OTHER  \n",
       "4922    SAD  \n",
       "\n",
       "[4923 rows x 2 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_excel('./train_fa2.xlsx')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¯ÙˆØ³ØªØ§Ù† Ø§Ø³Ù¾ÛŒØ³ Ù¾Ø±ÛŒØ¯ Ú†ÙˆÙ† Ø¬ÙˆØ²Ù Ø®ÙˆØ§Ø¨Ø´ Ø¨Ø±Ø¯ ?\n",
      "OTHER\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 4923)\n",
    "print(df4.loc[i, 'Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†'])\n",
    "print(df4.loc[i, 'SAD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø­Ù„Ù‡ØŒ Ø­Ø§Ù„Ø§ Ú©Ù‡ Ø¯ÛŒØªØ§ Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØŒ Ø¨Ø±ÛŒÙ… Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ú†ÛŒÚ©Ø§Ø± Ø¨Ø§ÛŒØ¯ Ø¨Ú©Ù†ÛŒ. Ø¬ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ù…ÛŒØ¯Ù… Ú©Ù‡ Ø§Ù†Ú¯Ø§Ø± Ø¯Ø§Ø±ÛŒ ÛŒÙ‡ Ø¨Ø§Ø²ÛŒ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯ÛŒ Ùˆ Ù‡Ø± Ù…Ø±Ø­Ù„Ù‡ Ø±Ùˆ Ø¨Ø§ÛŒØ¯ Ø¯Ø±Ø³Øª Ø±Ø¯ Ú©Ù†ÛŒ ØªØ§ Ø¨Ø±ÛŒ Ø¨Ø¹Ø¯ÛŒ.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª** (Ù…Ø«Ù„ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†Ù‚Ø´Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø²ÛŒ)  \n",
    "Ù‚Ø¨Ù„ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ø± Ú©Ø§Ø±ÛŒ Ø¨Ú©Ù†ÛŒØŒ Ø¨Ø§ÛŒØ¯ ÛŒÙ‡ Ù†Ú¯Ø§Ù‡ Ú©Ù„ÛŒ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø³ØªØª Ø¨Ù†Ø¯Ø§Ø²ÛŒ. Ø§ÛŒÙ† Ú©Ø§Ø± Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ú©Ù‡ Ø¨ÙÙ‡Ù…ÛŒ:  \n",
    "- **Ú†Ù†Ø¯ ØªØ§ Ø¯Ø§Ø¯Ù‡ Ø¯Ø§Ø±ÛŒØŸ** (Ø®Ø¨ Ú¯ÙØªÛŒ Û±Û³,Û°Û°Û° ØªØ§ØŒ Ø§ÙˆÚ©ÛŒ)  \n",
    "- **Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú†ÛŒ Ù‡Ø³ØªÙ†ØŸ** (Ù…Ø«Ù„Ø§Ù‹ØŒ Ø³ØªÙˆÙ† Ù…ØªÙ† ØªÙˆÛŒÛŒØªØŒ Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ...)  \n",
    "- **Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú†Ø·ÙˆØ±Ù‡ØŸ** (Ù…Ø«Ù„Ø§Ù‹ØŒ Ù†Ú©Ù†Ù‡ Û¹Û°Ùª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ \"Ø®ÙˆØ´Ø­Ø§Ù„\" Ø¨Ø§Ø´Ù† Ùˆ ÙÙ‚Ø· Û±Û°Ùª Ø¨Ù‚ÛŒÙ‡ØŸ Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ù…ÛŒØ´Ù‡)  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # Ø§ÛŒÙ†Ùˆ Ø¨Ø§ Ø§Ø³Ù… ÙØ§ÛŒÙ„Øª Ø¹ÙˆØ¶ Ú©Ù†\n",
    "print(df.head())  # Ù†Ù…Ø§ÛŒØ´ Ûµ Ø³Ø·Ø± Ø§ÙˆÙ„\n",
    "print(df.info())  # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¯ÛŒØªØ§Ø³Øª\n",
    "print(df['label'].value_counts())  # Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "```\n",
    "âœ **Ø§Ú¯Ø± ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ oversampling ÛŒØ§ undersampling Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ (Ø¨Ø¹Ø¯Ø§Ù‹ Ø¨Ù‡Ø´ Ù…ÛŒâ€ŒØ±Ø³ÛŒÙ…).**  \n",
    "\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§ Ø¯Ø± Ù¾Ø§Ù†Ø¯Ø§Ø³](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” **Ù…Ø±Ø­Ù„Ù‡ Û²: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Data Cleaning)** (Ù…Ø«Ù„ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¨Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹)  \n",
    "Ø¨Ø§ÛŒØ¯ Ù…Ø·Ù…Ø¦Ù† Ø¨Ø´ÛŒ Ú©Ù‡ Ø¯ÛŒØªØ§ÛŒ Ø®Ø§Ù…Øª Ù…Ø´Ú©Ù„ Ø®Ø§ØµÛŒ Ù†Ø¯Ø§Ø±Ù‡. Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬ Ø§ÛŒÙ†Ø§ Ù‡Ø³ØªÙ†:  \n",
    "âœ… **Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ (Missing Values)**  \n",
    "âœ… **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ (Duplicate Data)**  \n",
    "âœ… **ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†ÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ø´Ø§Ù…Ù„ ÙÙ‚Ø· @ Ùˆ # Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ù…ØªÙ† Ù…ÙÛŒØ¯)**  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ:**  \n",
    "```python\n",
    "# Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ\n",
    "df = df.dropna()\n",
    "\n",
    "# Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± ØªÚ©Ø±Ø§Ø±ÛŒ\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ù„ÛŒÙ†Ú© Ùˆ Ù…Ù†Ø´Ù† Ù‡Ø³ØªÙ†\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø¢Ù…ÙˆØ²Ø´ Data Cleaning Ø¯Ø± Ù¾Ø§Ù†Ø¯Ø§Ø³](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›  **Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† (Text Preprocessing)** (Ù…Ø«Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ø§Ø±Ø§Ú©ØªØ± ØªÙˆÛŒ Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø§Ù„Ø§Ù† Ø¨Ø§ÛŒØ¯ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÛŒÛŒØª Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ø«ÛŒÙ Ù‡Ø³ØªÙ† Ùˆ Ù¾Ø± Ø§Ø² Ú†ÛŒØ²Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ù…Ø«Ù„:  \n",
    "âŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ (https://...)  \n",
    "âŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ (@user)  \n",
    "âŒ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ (#Ù…ÙˆØ¶ÙˆØ¹)  \n",
    "âŒ Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ (ğŸ˜‚ğŸ˜¡â¤ï¸)  \n",
    "âŒ Ø­Ø±ÙˆÙ Ø§Ø¶Ø§ÙÛŒ Ú©Ø´ÛŒØ¯Ù‡ (\"Ø¹Ø§Ø§Ø§Ø§Ù„ÛŒÛŒÛŒÛŒ\")  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # Ø­Ø°Ù Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # Ø­Ø°Ù Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š **Ù…Ø±Ø­Ù„Ù‡ Û´: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Exploratory Data Analysis - EDA)** (Ù…Ø«Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹)  \n",
    "Ø¨Ø§ÛŒØ¯ ÛŒÙ‡ Ú©Ù… Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Øª Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ Ú©Ù‡ Ø¨ÙÙ‡Ù…ÛŒ:  \n",
    "- **Ú©Ø¯ÙˆÙ… Ú©Ù„Ù…Ø§Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù†ØŸ**  \n",
    "- **Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø§Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ**  \n",
    "- **Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú†Ù‚Ø¯Ø± ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù†ØŸ**  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª Ù¾Ø± ØªÚ©Ø±Ø§Ø±:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[EDA Ø¯Ø± NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ **Ù…Ø±Ø­Ù„Ù‡ Ûµ: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¹Ø¯Ø¯ (Tokenization & Embedding)** (Ù…Ø«Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù† Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª)  \n",
    "Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ù…ÛŒâ€ŒÙÙ‡Ù…Ù†ØŒ Ù¾Ø³ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø±Ùˆ Ø¨Ù‡ Ø¹Ø¯Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…. Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡:  \n",
    "- **Bag of Words (BoW)** â€“ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ú©Ù‡ ÙÙ‚Ø· ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ùˆ Ù…ÛŒâ€ŒØ´Ù…Ø±Ù‡  \n",
    "- **TF-IDF** â€“ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± Ù…ØªÙ†  \n",
    "- **Word Embeddings (Ù…Ø«Ù„ Word2Vec ÛŒØ§ BERT)** â€“ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ  \n",
    "\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² BERT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ù…Ù‚Ø¯Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø± Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ— **Ù…Ø±Ø­Ù„Ù‡ Û¶: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ (Training a Model)** (Ù…Ø«Ù„ Ø³Ø§Ø®ØªÙ† Ø´Ø®ØµÛŒØª Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø¨Ø§ÛŒØ¯ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±ÛŒ Ú©Ù‡ Ø§Ø² Ú†Ù‡ Ù…Ø¯Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:  \n",
    "âœ… **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ (Ù…Ø«Ù„ Naive Bayes, SVM)** â€“ Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø³Øª Ú©ÙˆÚ†ÛŒÚ© Ø¨Ø§Ø´Ù‡  \n",
    "âœ… **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± (Ù…Ø«Ù„ LSTM, BERT)** â€“ Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ø´Ù‡ Ùˆ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ø¨Ø®ÙˆØ§ÛŒ  \n",
    "\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² BERT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ BERT Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Ù…Ø±Ø­Ù„Ù‡ Û·: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ (Evaluation & Testing)** (Ù…Ø«Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ØŒ Ø¨Ø§ÛŒØ¯ Ø¨Ø¨ÛŒÙ†ÛŒ Ú†Ù‚Ø¯Ø± Ø®ÙˆØ¨ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ù‡. Ø§Ø² Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ **Ø¯Ù‚Øª (Accuracy)ØŒ Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision)ØŒ ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall)** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Ø­Ø§Ù„Ø§ ØªÙˆ Ø¨Ú¯ÙˆØŒ ØªÙˆ Ú©Ø¯ÙˆÙ… Ù…Ø±Ø­Ù„Ù‡ Ø³ÙˆØ§Ù„ Ø¯Ø§Ø±ÛŒØŸ** ğŸ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ø®Ø§Ø·Ø± Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ† Ø¬ÙˆØ§Ù†ÛŒ Ù…Ø§Ù† ØŒ Ø§Ù…ÛŒØ¯Ù…Ø§Ù† ØŒ Ø§Ù†Ø±Ú˜ÛŒ Ùˆ Ù†Ø´Ø§Ø· Ù…Ø§Ù†..\n",
      "Ø¨Ø®Ø§Ø·Ø± Ø®ÙˆÙ† Ù‡Ø§ÛŒ Ø±ÛŒØ®ØªÙ‡ Ø´Ø¯Ù‡ Û·Û¸  Û¸Û¸  Û¹Û¶  Û¹Û¸ \n",
      "Ø¨Ø®Ø§Ø·Ø± Ù…Ø§Ø¯Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù„ Ùˆ Ú†Ø´Ù…Ø´ÙˆÙ† Ø®ÙˆÙ†Ù‡ \n",
      "Ø¨Ø®Ø§Ø·Ø± Ù¾Ø¯Ø± Ù…Ø§Ø¯Ø± Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¬ÙˆÙˆÙ† ØºØ±Ù‚ Ø¨Ù‡ Ø®ÙˆÙ†Ø´ÙˆÙ† Ùˆ Ø¨Ù‡ Ø®Ø§Ú© Ø³Ù¾Ø±Ø¯Ù†.\n",
      "Ø±Ø§ÛŒ Ø¨ÛŒ Ø±Ø§ÛŒ \n",
      "#No2IR\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø´Ø§Ø¯ Ø¨Ø§ \"happy\" ÛŒØ§ \"joyful\" Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒØ´Ù†\n",
    "happy_tweets = df[df['emotion'].isin(['joy'])]\n",
    "\n",
    "# Ø§Ù†ØªØ®Ø§Ø¨ ÛŒÚ© Ø±Ø¯ÛŒÙ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø¯\n",
    "i = random.randint(0, len(happy_tweets) - 1)\n",
    "print(happy_tweets.iloc[i]['tweet'])\n",
    "print(happy_tweets.iloc[i]['emotion'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i= random.randint(0, 113829)\n",
    "# print(df.loc[i, 'tweet'])\n",
    "# print(df.loc[i, 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113829 entries, 0 to 113828\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   tweet         113829 non-null  object\n",
      " 1   replyCount    113829 non-null  int64 \n",
      " 2   retweetCount  113829 non-null  int64 \n",
      " 3   likeCount     113829 non-null  int64 \n",
      " 4   quoteCount    113829 non-null  int64 \n",
      " 5   hashtags      113829 non-null  object\n",
      " 6   sourceLabel   113829 non-null  object\n",
      " 7   emotion       113829 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n",
      "emotion\n",
      "sad         30.157517\n",
      "joy         24.619385\n",
      "anger       17.630832\n",
      "fear        15.482873\n",
      "surprise    11.296770\n",
      "disgust      0.812622\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of labels\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "print(df['emotion'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Repetitive Rows\n",
    "#df= df.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
