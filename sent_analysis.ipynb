{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Twitter Dataset\n",
    "About Dataset\n",
    "\n",
    "This dataset contains more than 3300 Persian tweets, crawled from Twitter.com\n",
    "Each tweet is assigned a label, which is a number between 0 to 4.\n",
    "Label 0 indicates the sentiment of Happiness and Joy.\n",
    "Label 1 indicates the sentiment of Sadness.\n",
    "Label 2 indicates the sentiment of Anger and Furiosity.\n",
    "Label 3 indicates the sentiment of Neutral.\n",
    "And finally, label 4 indicates the sentiment of intense emotions, such as Surprise, Fear, and Love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Numeric Labels</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù… ØºØ§ÛŒØª Ø²Ù†Ø¯Ú¯ÛŒ Ù…Ù† Ø§ÛŒÙ†Ù‡ Ú©Ù‡ ÛŒÚ©ÛŒ ÛŒØ±ÙˆØ² Ø¨Ù...</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ø§ÛŒ Ø¨Ø§Ø¨Ø§ Ù†ØµÛŒØ¨ Ù‡Ø±Ú©Ø³ÛŒ Ù†Ù…ÛŒØ´Ù‡ Ú©Ù‡ØŒ ÙÙ‚Ø· Ø¯Ø¹Ø§ÛŒ Ø®ÛŒØ± Ù¾Ø¯Ø± ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ø¨Ø§ Ù…Ù† Ù…Ø«Ù„ Ø¨Ù‚ÛŒÙ‡ Ø±ÙØªØ§Ø± Ù†Ú©Ù† Ø¢Ø´ØºØ§Ù„</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ù…Ù† Ø¹Ù„Ø§Ù‚Ù‡ Ø´Ø¯ÛŒØ¯ÛŒ Ø¨Ù‡ Ø´Ù†ÛŒØ¯Ù† Â«Ø¨ØºÙ„Ù… Ú©Ù†Â» Ø§Ø² Ø·Ø±Ù Ø§ÙˆÙ†ÛŒ ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ú†ÛŒØ² Ú©ÛŒÚ© Ú¯Ø±Ù… Ùˆ Ù†Ø±Ù… Ù…ÛŒØ®ÙˆØ§Ù….</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>Ù…Ù…Ù„Ú©Øª Ù†ÛŒØ³Øª Ú©Ù‡ ØªÙˆÙ†Ù„ ÙˆØ­Ø´ØªÙ‡. Ø¨Ú†Ù‡ Ø±Ùˆ Ù…ÛŒØ¨Ø±Ù† Ù‚Ø§ØªÙ„ Ø¨Ø§...</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ Ø§Ù…ØªØ­Ø§Ù†Ø§ Ú©Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯ Ù…Ø«Ù„Ø§ ÛŒÚ©Ù… Ú©ØµÚ©Ù„Ú© Ú©Ù†Ù…...</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>Ø¯ÙˆØ³ØªØ§Ù† Ø§ÙˆÙ† Ø³Ù‡ Ù†Ù‚Ø·Ù‡ Ù‚Ø±Ù…Ø² Ù‡Ø§ Ù‡Ø³Øª Ú©Ù‡ Ù…ÛŒØ²Ù†ÛŒ Ø¨Ø§Ø² Ù…ÛŒ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>Ù†ØªÛŒØ¬Ù‡ Ø±ÛŒØ³Ø±Ú† Ú©Ù…â€ŒÚ©Ù… Ø¯Ø§Ø±Ù‡ Ø¨Ù‡ Ø³Ù…ØªÛŒ Ø®ÙˆØ¨ÛŒ Ù¾ÛŒØ´ Ù…ÛŒØ±Ù‡</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>3380</td>\n",
       "      <td>Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡â€ŒÙ‡Ø§. Ú†Ù‡ Ø®Ø¨Ø±Ø§ØŸ</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3381 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             Tweets  \\\n",
       "0              0  Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù… ØºØ§ÛŒØª Ø²Ù†Ø¯Ú¯ÛŒ Ù…Ù† Ø§ÛŒÙ†Ù‡ Ú©Ù‡ ÛŒÚ©ÛŒ ÛŒØ±ÙˆØ² Ø¨Ù...   \n",
       "1              1  Ø§ÛŒ Ø¨Ø§Ø¨Ø§ Ù†ØµÛŒØ¨ Ù‡Ø±Ú©Ø³ÛŒ Ù†Ù…ÛŒØ´Ù‡ Ú©Ù‡ØŒ ÙÙ‚Ø· Ø¯Ø¹Ø§ÛŒ Ø®ÛŒØ± Ù¾Ø¯Ø± ...   \n",
       "2              2                     Ø¨Ø§ Ù…Ù† Ù…Ø«Ù„ Ø¨Ù‚ÛŒÙ‡ Ø±ÙØªØ§Ø± Ù†Ú©Ù† Ø¢Ø´ØºØ§Ù„   \n",
       "3              3  Ù…Ù† Ø¹Ù„Ø§Ù‚Ù‡ Ø´Ø¯ÛŒØ¯ÛŒ Ø¨Ù‡ Ø´Ù†ÛŒØ¯Ù† Â«Ø¨ØºÙ„Ù… Ú©Ù†Â» Ø§Ø² Ø·Ø±Ù Ø§ÙˆÙ†ÛŒ ...   \n",
       "4              4                          Ú†ÛŒØ² Ú©ÛŒÚ© Ú¯Ø±Ù… Ùˆ Ù†Ø±Ù… Ù…ÛŒØ®ÙˆØ§Ù….   \n",
       "...          ...                                                ...   \n",
       "3376        3376  Ù…Ù…Ù„Ú©Øª Ù†ÛŒØ³Øª Ú©Ù‡ ØªÙˆÙ†Ù„ ÙˆØ­Ø´ØªÙ‡. Ø¨Ú†Ù‡ Ø±Ùˆ Ù…ÛŒØ¨Ø±Ù† Ù‚Ø§ØªÙ„ Ø¨Ø§...   \n",
       "3377        3377  Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯ Ø§Ù…ØªØ­Ø§Ù†Ø§ Ú©Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯ Ù…Ø«Ù„Ø§ ÛŒÚ©Ù… Ú©ØµÚ©Ù„Ú© Ú©Ù†Ù…...   \n",
       "3378        3378  Ø¯ÙˆØ³ØªØ§Ù† Ø§ÙˆÙ† Ø³Ù‡ Ù†Ù‚Ø·Ù‡ Ù‚Ø±Ù…Ø² Ù‡Ø§ Ù‡Ø³Øª Ú©Ù‡ Ù…ÛŒØ²Ù†ÛŒ Ø¨Ø§Ø² Ù…ÛŒ...   \n",
       "3379        3379      Ù†ØªÛŒØ¬Ù‡ Ø±ÛŒØ³Ø±Ú† Ú©Ù…â€ŒÚ©Ù… Ø¯Ø§Ø±Ù‡ Ø¨Ù‡ Ø³Ù…ØªÛŒ Ø®ÙˆØ¨ÛŒ Ù¾ÛŒØ´ Ù…ÛŒØ±Ù‡    \n",
       "3380        3380                              Ø³Ù„Ø§Ù… Ø¨Ú†Ù‡â€ŒÙ‡Ø§. Ú†Ù‡ Ø®Ø¨Ø±Ø§ØŸ   \n",
       "\n",
       "      Numeric Labels             Label  \n",
       "0                  4  Intense Emotions  \n",
       "1                  0             Happy  \n",
       "2                  2             Angry  \n",
       "3                  4  Intense Emotions  \n",
       "4                  0             Happy  \n",
       "...              ...               ...  \n",
       "3376               2             Angry  \n",
       "3377               2             Angry  \n",
       "3378               0             Happy  \n",
       "3379               0             Happy  \n",
       "3380               3           Neutral  \n",
       "\n",
       "[3381 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2= pd.read_csv('./PersianTwitterDataset.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒÙ† Ø­ÛŒØ¯Ø± Ø­ÛŒØ¯Ø± Ú†Ù‚Ø¯Ø± Ù…ÛŒÚ©Ø³ Ø²ÛŒØ¨Ø§ Ùˆ Ø¯Ù‚ÛŒÙ‚ÛŒÙ‡ğŸ«¶\n",
      "Intense Emotions\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 3300)\n",
    "print(df2.loc[i, 'Tweets'])\n",
    "print(df2.loc[i, 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian tweets emotional dataset\n",
    "About Dataset\n",
    "New Persian Dataset\n",
    "Since Persian datasets are really scarce I scrape Twitter in order to make a new Persian dataset.\n",
    "\n",
    "The tweets have been pulled from Twitter using snscrape and manual tagging has been done based on Ekman's 6 main emotions.\n",
    "For privacy sake, I pre-process and remove usernames, display names, and mentions from all tweets. Also, I deleted the timestamps and Tweets IDs.\n",
    "\n",
    "Columns:\n",
    "1) tweet\n",
    "2) replyCount\n",
    "3) retweetCount\n",
    "4) likeCount\n",
    "5) quoteCount\n",
    "6) hashtags\n",
    "7) sourceLabel\n",
    "8) emotion\n",
    "\n",
    "Please leave an upvote if you find this relevant. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "sad         34328\n",
       "joy         28024\n",
       "anger       20069\n",
       "fear        17624\n",
       "surprise    12859\n",
       "disgust       925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('./merged.csv')\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ù… Ø¯Ù…Ø§ÛŒ ØµØ¨Ø­ Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù…</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ú©ÛŒÙÛŒØªØ´ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ø¨Ø§ Ø´Ú© Ø®Ø±ÛŒØ¯Ù… ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒÙ…...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú†ÙˆÙ† Ù‡Ù…Ø´ Ø¨Ø§ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø«Ø¨Øª Ø´Ø¯Ù‡ ØŒ Ø§ÛŒØ§ Ù…ÛŒØ´Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶ Ø²Ø¯...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§ÙŠÙ† ÙˆØ¶Ø¹ Ø¨ Ø·Ø±Ø² Ø®Ù†Ø¯Ù‡ Ø¯Ø§Ø±ÙŠ Ú¯Ø±ÙŠÙ‡ Ø¯Ø§Ø±Ù‡ ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø®Ø¨ Ù…Ù† Ø±Ø³Ù…Ø§ Ø§Ø² ÛŒÚ© Ù†ÙØ± Ù…ØªÙ†ÙØ±Ù…ØŒÚ†ÙˆÙ† Ø§Ø² Ú¯Ø±Ø¨Ù‡ Ø¨Ø¯Ø´ Ù…ÛŒ...</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>Ù…Ø±Ø­ÙˆÙ… Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø¢Ø¨Ú©ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒÚ©Ø±Ø¯     Ù…Ø±Ø­ÙˆÙ… Ø¹Ø¬Ø¨ Ø¢...</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>Ú©Ù„Ø§ Ø¹ÛŒÙ† Ø§Ø¹ØªÙ‚Ø§Ø¯Ø§Øª Ùˆ ØªÙˆØ¦ÛŒØª Ø²Ø¯Ù†Ø§ØªÙˆÙ† ... !!   Ø¯Ø± Ù‚...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>Ø®Ø¨ ÙˆÙ‚ØªÛŒ Ù…ÛŒÚ¯ÛŒ Ú©Ø³ÛŒ Ø¨ÛŒØ§Ø¯ Ù…Ø§Ø±Ùˆ Ø¨Ú¯ÛŒØ±Ù‡ ÛŒØ§Ø±Ùˆ ØªØ±Ø³ Ù…ÛŒÚ©Ù†...</td>\n",
       "      <td>FEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>Ù‡Ù…ÙˆÙ† Ù‡Ø§Ø±Ùˆ     Ù…Ú¯Ù‡ Ø¢Ù‡Ù†Ú¯ Ø¬Ø¯ÛŒØ¯Ø§ÛŒ Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‡Ù‡ ...</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>Ù†ÛŒÙ… Ø¯Ú¯ÛŒØ±Ø´ Ú†Ø·ÙˆØ± Ø­Ù„ Ù†ÛŒØ´Ø¯</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet   emotion\n",
       "0        Ø§Ø² ØµØ¯Ø§ÛŒ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ù… Ø¯Ù…Ø§ÛŒ ØµØ¨Ø­ Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù… Ù…ØªÙ†ÙØ±Ù…      HATE\n",
       "1      \"Ú©ÛŒÙÛŒØªØ´ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡ Ø¨Ø§ Ø´Ú© Ø®Ø±ÛŒØ¯Ù… ÙˆÙ„ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§Ø¶ÛŒÙ…...       SAD\n",
       "2     Ú†ÙˆÙ† Ù‡Ù…Ø´ Ø¨Ø§ Ø¯ÙˆØ±Ø¨ÛŒÙ† Ø«Ø¨Øª Ø´Ø¯Ù‡ ØŒ Ø§ÛŒØ§ Ù…ÛŒØ´Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶ Ø²Ø¯...     OTHER\n",
       "3                 Ø§ÙŠÙ† ÙˆØ¶Ø¹ Ø¨ Ø·Ø±Ø² Ø®Ù†Ø¯Ù‡ Ø¯Ø§Ø±ÙŠ Ú¯Ø±ÙŠÙ‡ Ø¯Ø§Ø±Ù‡ ...       SAD\n",
       "4     Ø®Ø¨ Ù…Ù† Ø±Ø³Ù…Ø§ Ø§Ø² ÛŒÚ© Ù†ÙØ± Ù…ØªÙ†ÙØ±Ù…ØŒÚ†ÙˆÙ† Ø§Ø² Ú¯Ø±Ø¨Ù‡ Ø¨Ø¯Ø´ Ù…ÛŒ...      HATE\n",
       "...                                                 ...       ...\n",
       "6151  Ù…Ø±Ø­ÙˆÙ… Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ø¢Ø¨Ú©ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒÚ©Ø±Ø¯     Ù…Ø±Ø­ÙˆÙ… Ø¹Ø¬Ø¨ Ø¢...  SURPRISE\n",
       "6152  Ú©Ù„Ø§ Ø¹ÛŒÙ† Ø§Ø¹ØªÙ‚Ø§Ø¯Ø§Øª Ùˆ ØªÙˆØ¦ÛŒØª Ø²Ø¯Ù†Ø§ØªÙˆÙ† ... !!   Ø¯Ø± Ù‚...     ANGRY\n",
       "6153  Ø®Ø¨ ÙˆÙ‚ØªÛŒ Ù…ÛŒÚ¯ÛŒ Ú©Ø³ÛŒ Ø¨ÛŒØ§Ø¯ Ù…Ø§Ø±Ùˆ Ø¨Ú¯ÛŒØ±Ù‡ ÛŒØ§Ø±Ùˆ ØªØ±Ø³ Ù…ÛŒÚ©Ù†...      FEAR\n",
       "6154  Ù‡Ù…ÙˆÙ† Ù‡Ø§Ø±Ùˆ     Ù…Ú¯Ù‡ Ø¢Ù‡Ù†Ú¯ Ø¬Ø¯ÛŒØ¯Ø§ÛŒ Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‡Ù‡ ...  SURPRISE\n",
       "6155                            Ù†ÛŒÙ… Ø¯Ú¯ÛŒØ±Ø´ Ú†Ø·ÙˆØ± Ø­Ù„ Ù†ÛŒØ´Ø¯      OTHER\n",
       "\n",
       "[6156 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_excel('./train_fa.xlsx')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamid\\AppData\\Local\\Temp\\ipykernel_31384\\2986185299.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df3 = df3[~df['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# drop the rows tha have nan values\n",
    "df3= df3.dropna()\n",
    "print(df3.isna().sum())\n",
    "\n",
    "# drop the duplicated values\n",
    "df3= df3.drop_duplicates()\n",
    "print(df3.duplicated().sum())\n",
    "df3\n",
    "\n",
    "# Filter rows that only contain links, @ or #\n",
    "df3 = df3[~df['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # Ø­Ø°Ù Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§ (@mentions)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # Ø­Ø°Ù Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ (#hashtags)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "    return text\n",
    "\n",
    "df3['clean_text'] = df3['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ú†Ù‡Ù‡Ø§ Ú©Ù‡ Ø´Ø§Ù†Ø³ Ø¢ÙˆØ±Ø¯Ù… Ø®ÙˆØ§Ø¨ Ù…ÙˆÙ†Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ù‡Ù†ÙˆØ² Ù†Ø±ÙØªÙ‡ Ø¨ÙˆØ¯ Û±Û³ Ø¨Ù‡ Ø¯Ø± Ø±ÙØªÛŒÙ… Ù…Ø±Ú©Ø² Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ ÙˆØ¬ÙˆØ¯ÛŒ Ú©Ù‡ Ù…Ø¬ÙˆØ² ØªØ±Ø¯Ø¯ Ø¯Ø§Ø´ØªÛŒÙ… Ø­Ø±Ø§Ø³Øª Ø±Ø§Ù‡Ù…ÙˆÙ† Ù†Ù…ÛŒØ¯Ø§Ø¯ Ø¨Ø§ Ú©Ù„ÛŒ Ø¨Ø¯Ø¨Ø®ØªÛŒ Ùˆ ØªÙ…Ø§Ø³ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ú©Ø±Ø¯ÛŒÙ… Ø±ÙØªÛŒÙ… Ø¯Ø§Ø®Ù„ Ù¾Ø§ÛŒ Ø³Ø±ÙˆØ± Û²Ûµ\n",
      "Processed Text: ['Ø¨Ú†Ù‡', 'Ø´Ø§Ù†Ø³', 'Ø¢ÙˆØ±Ø¯Ù…', 'Ø®ÙˆØ§Ø¨', 'Ù…ÙˆÙ†Ø¯Ù‡_Ø¨ÙˆØ¯', 'Ù†Ø±ÙØªÙ‡_Ø¨ÙˆØ¯', 'Ø±ÙØªÛŒÙ…', 'Ù…Ø±Ú©Ø²', 'ÙˆØ¬ÙˆØ¯ÛŒ', 'Ù…Ø¬ÙˆØ²', 'ØªØ±Ø¯Ø¯', 'Ø¯Ø§Ø´Øª', 'Ø­Ø±Ø§Ø³Øª', 'Ø±Ø§Ù‡Ù…ÙˆÙ†', 'Ù†Ù…ÛŒØ¯Ø§Ø¯', 'Ø¨Ø¯Ø¨Ø®ØªÛŒ', 'ØªÙ…Ø§Ø³', 'Ù‡Ù…Ø§Ù‡Ù†Ú¯', 'Ú©Ø±Ø¯ÛŒÙ…', 'Ø±ÙØªÛŒÙ…', 'Ø¯Ø§Ø®Ù„', 'Ù¾Ø§ÛŒ', 'Ø³Ø±ÙˆØ±']\n"
     ]
    }
   ],
   "source": [
    "from hazm import Normalizer, WordTokenizer, stopwords_list, Lemmatizer\n",
    "import random\n",
    "import string\n",
    "\n",
    "def preprocessing(text, Apply_normalizer= True, Aplly_wordtokenizer= True, remove_stop_words= True, Apply_lemmatizer=True, remove_numbers=True):\n",
    "  '''\n",
    "  Cleaning and preprocessing the given text.\n",
    "\n",
    "  Args:\n",
    "    text (str): the input text that we want to work on in.\n",
    "    Apply_normalizer (bool): if True, shows that text will bee normalize by hazm. Defualt is True\n",
    "    Aplly_wordtokenizer (bool): if True, the text will tokenize. Defualt is True\n",
    "    remove_stop_words (list): a list that have the set of stopwords in pesian. Defualt is True\n",
    "    Apply_lemmatizer (bool): change the word to its root. Defualt is True\n",
    "    remove_numbers (bool): remove the numbers from text\n",
    "    replace the \\u200c:( text = \"Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ù… Ú©ØªØ§Ø¨ÛŒ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡ Ùˆ Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù† Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù….\"\n",
    "                        processed_text = ['Ù…ÛŒ\\u200cØ®ÙˆØ§Ù†Ù…', 'Ú©ØªØ§Ø¨ÛŒ', 'Ø§Ø²', 'Ú©ØªØ§Ø¨\\u200cØ®Ø§Ù†Ù‡', 'Ùˆ', 'Ø¯Ø§Ù†Ø´\\u200cØ¢Ù…ÙˆØ²Ø§Ù†', 'Ø±Ø§', 'Ù…ÛŒ\\u200cØ¨ÛŒÙ†Ù…'])\n",
    "\n",
    "\n",
    "  Return:\n",
    "    filtered (list): list of words(of text) after cleaning and preprocessing.\n",
    "  '''\n",
    "\n",
    "  \n",
    "  # Initinalize requiared tools from Hazm\n",
    "  normalizer= Normalizer()\n",
    "  wordtokenizer= WordTokenizer()\n",
    "  stopword_lst= stopwords_list()\n",
    "  lemmatizer = Lemmatizer()\n",
    "  panctuation= string.punctuation +  \"ØŸØŒØ›.,#\"\n",
    "  \n",
    "\n",
    "\n",
    "  # Step 1 : Normalize the text\n",
    "  if Apply_normalizer:\n",
    "    text= normalizer.normalize(text)\n",
    "\n",
    "\n",
    "    \n",
    "  # Step 2: Tokenize the text to words\n",
    "  if Aplly_wordtokenizer:\n",
    "    words= wordtokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "  # Step 3: delete the aditional numbers and words\n",
    "  if remove_numbers:\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "\n",
    "\n",
    "  # step 4 : Delet the stop words from  words(list)\n",
    "  if remove_stop_words:\n",
    "    words= [word for word in words if word not in stopword_lst]\n",
    "\n",
    "\n",
    "\n",
    "  # Step 5: delete the additionala (.?!,)\n",
    "  if panctuation:\n",
    "    words= [word for word in words if word not in panctuation]  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  #step 6: find the root of words\n",
    "  if Apply_lemmatizer:\n",
    "    words= [lemmatizer.lemmatize(word, pos= 'v') for word in words]\n",
    "\n",
    "\n",
    "  #ÙStep 7: replace the '\\u200c'  \n",
    "  filtered= [word.replace('\\u200c', '')for word in words]\n",
    "\n",
    "  return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Kodun ne yaptÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in ğŸ˜Š\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x= random.randint(1, 6000)\n",
    "    sample_text = df3['clean_text'][x]\n",
    "    processed_text = preprocessing(\n",
    "        sample_text)\n",
    "    print(\"Original Text:\", sample_text)\n",
    "    print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessing)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[65], line 26\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(text, Apply_normalizer, Aplly_wordtokenizer, remove_stop_words, Apply_lemmatizer, remove_numbers)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mCleaning and preprocessing the given text.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m  filtered (list): list of words(of text) after cleaning and preprocessing.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Initinalize requiared tools from Hazm\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m normalizer\u001b[38;5;241m=\u001b[39m Normalizer()\n\u001b[0;32m     27\u001b[0m wordtokenizer\u001b[38;5;241m=\u001b[39m WordTokenizer()\n\u001b[0;32m     28\u001b[0m stopword_lst\u001b[38;5;241m=\u001b[39m stopwords_list()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\normalizer.py:171\u001b[0m, in \u001b[0;36mNormalizer.__init__\u001b[1;34m(self, correct_spacing, remove_diacritics, remove_specials_chars, decrease_repeated_chars, persian_style, persian_numbers, unicodes_replacement, seperate_mi)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecials_chars_patterns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;66;03m# Remove almoast all arabic unicode superscript and subscript characters in the ranges of 00600-06FF, 08A0-08FF, FB50-FDFF, and FE70-FEFF\u001b[39;00m\n\u001b[0;32m    164\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         ),\n\u001b[0;32m    168\u001b[0m     ]\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seperate_mi:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbs \u001b[38;5;241m=\u001b[39m Lemmatizer(joined_verb_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mverbs\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_mi_patterns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbÙ†?Ù…ÛŒ[Ø¢Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ]+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unicodes_replacement:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\lemmatizer.py:49\u001b[0m, in \u001b[0;36mLemmatizer.__init__\u001b[1;34m(self, words_file, verbs_file, joined_verb_parts)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer \u001b[38;5;241m=\u001b[39m Stemmer()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconjugation \u001b[38;5;241m=\u001b[39m Conjugation()\n\u001b[1;32m---> 49\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WordTokenizer(words_file\u001b[38;5;241m=\u001b[39mdefault_words, verbs_file\u001b[38;5;241m=\u001b[39mverbs_file)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbs_file:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\word_tokenizer.py:103\u001b[0m, in \u001b[0;36mWordTokenizer.__init__\u001b[1;34m(self, words_file, verbs_file, join_verb_parts, join_abbreviations, separate_emoji, replace_links, replace_ids, replace_emails, replace_numbers, replace_hashtags)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# NOTE: python2.7 does not support unicodes with \\w\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhashtag_repl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAG \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;241m0\u001b[39m]: (item[\u001b[38;5;241m1\u001b[39m], item[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m words_list(words_file)}\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join_verb_parts:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_verbs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mØ§Ù…\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mØ§ÛŒ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÙ†Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\utils.py:48\u001b[0m, in \u001b[0;36mwords_list\u001b[1;34m(words_file)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[0;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     49\u001b[0m         (item[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\utils.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[0;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     49\u001b[0m         (item[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df3[\"clean_text\"] = df3[\"clean_text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¹Ú©Ø³ÛŒ Ú©Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ² Ù¾ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø§Ù„Ø§Ù†Ø´...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ØªÙ†Ù‡Ø§ÛŒÛŒÙ… Ø´Ø¨ÛŒÙ‡ ØªÙ†Ù‡Ø§ÛŒÛŒÙ‡ Ø¸Ù‡Ø±Ø§ÛŒ Ø¨Ú†Ú¯ÛŒÙ… Ø´Ø¯Ù‡ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù‡...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø®ÙˆØ¨Ù‡ ØªÙ…Ø§Ù… Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙˆØ´ÛŒ Ø±Ùˆ Ù…Ø­Ø§ÙØ¸Øª Ù…ÛŒâ€ŒÚ©Ù†Ù‡</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§ÛŒÙ† Ø®Ø§Ú© Ù…Ø§Ù„ Ù…Ø±Ø¯Ù…Ø§Ù† Ø§Ø³Øª Ù†Ù‡ Ø­Ø§Ú©Ù…Ø§Ù† #Ø§ÛŒØ±Ø§Ù† #Ù…Ù‡Ø³Ø§_...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§Ú¯Ù‡ ØªÙˆ Ø¨ØºÙ„Øª Ø¨ÙˆØ¯Ù… Ø­Ø§Ù„Ù… Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒØ´Ø¯</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>Ù…Ù† Ø§Ø² Ø¨Ùˆ Ùˆ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±ÛŒØ´ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… ØŒ Ù‚ÛŒÙ…ØªØ´ Ù‡Ù…â€Œ Ù…Ù†Ø§Ø³Ø¨Ù‡</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>Ú¯Ø§Ø² Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¢Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø±Ù‚ Ù†Ø¯Ø§Ø±ÛŒÙ… Ù†Øª Ù†Ø¯Ø§Ø±ÛŒÙ… Ù¾ÙˆÙ„ ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>ÛŒÚ©ÛŒ Ø¨Ù‡Ù… Ú¯ÙØª Ø¨Ø±Ù†Ùˆ Ú†Ø±Ø§ Ø¹Ø§Ø´Ù‚ Ù†Ù…ÛŒØ´ÛŒ Ú¯ÙØªÙ… Ù…Ø§ Ù¾ÙˆÙ„ Ø¹Ø§...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ù‚Ø¶ÛŒÙ‡ ÛŒ Ú¯Ø§Ø² Ù…ÛŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ… ÙÙ‚Ø· ÙØ±Ø§Ø®ÙˆØ§...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>Ø³Ù„Ø§Ù…. Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ§Ø¸Ø¨Øª Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ÙˆÛŒØ±ÙˆØ³ Ú©ÙˆÙØªÛŒ Ø±Ùˆâ€Œ Ù†Ú¯ÛŒ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4923 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†    SAD\n",
       "0     Ø¹Ú©Ø³ÛŒ Ú©Ù‡ Ú†Ù†Ø¯ Ø±ÙˆØ² Ù¾ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø§Ù„Ø§Ù†Ø´...  OTHER\n",
       "1     ØªÙ†Ù‡Ø§ÛŒÛŒÙ… Ø´Ø¨ÛŒÙ‡ ØªÙ†Ù‡Ø§ÛŒÛŒÙ‡ Ø¸Ù‡Ø±Ø§ÛŒ Ø¨Ú†Ú¯ÛŒÙ… Ø´Ø¯Ù‡ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ù‡...    SAD\n",
       "2              Ø®ÙˆØ¨Ù‡ ØªÙ…Ø§Ù… Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙˆØ´ÛŒ Ø±Ùˆ Ù…Ø­Ø§ÙØ¸Øª Ù…ÛŒâ€ŒÚ©Ù†Ù‡  HAPPY\n",
       "3     Ø§ÛŒÙ† Ø®Ø§Ú© Ù…Ø§Ù„ Ù…Ø±Ø¯Ù…Ø§Ù† Ø§Ø³Øª Ù†Ù‡ Ø­Ø§Ú©Ù…Ø§Ù† #Ø§ÛŒØ±Ø§Ù† #Ù…Ù‡Ø³Ø§_...  ANGRY\n",
       "4                  Ø§Ú¯Ù‡ ØªÙˆ Ø¨ØºÙ„Øª Ø¨ÙˆØ¯Ù… Ø­Ø§Ù„Ù… Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒØ´Ø¯    SAD\n",
       "...                                                 ...    ...\n",
       "4918  Ù…Ù† Ø§Ø² Ø¨Ùˆ Ùˆ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±ÛŒØ´ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… ØŒ Ù‚ÛŒÙ…ØªØ´ Ù‡Ù…â€Œ Ù…Ù†Ø§Ø³Ø¨Ù‡  HAPPY\n",
       "4919  Ú¯Ø§Ø² Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¢Ø¨ Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø±Ù‚ Ù†Ø¯Ø§Ø±ÛŒÙ… Ù†Øª Ù†Ø¯Ø§Ø±ÛŒÙ… Ù¾ÙˆÙ„ ...    SAD\n",
       "4920  ÛŒÚ©ÛŒ Ø¨Ù‡Ù… Ú¯ÙØª Ø¨Ø±Ù†Ùˆ Ú†Ø±Ø§ Ø¹Ø§Ø´Ù‚ Ù†Ù…ÛŒØ´ÛŒ Ú¯ÙØªÙ… Ù…Ø§ Ù¾ÙˆÙ„ Ø¹Ø§...    SAD\n",
       "4921  Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù‡ Ù‚Ø¶ÛŒÙ‡ ÛŒ Ú¯Ø§Ø² Ù…ÛŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ… ÙÙ‚Ø· ÙØ±Ø§Ø®ÙˆØ§...  OTHER\n",
       "4922  Ø³Ù„Ø§Ù…. Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ§Ø¸Ø¨Øª Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ÙˆÛŒØ±ÙˆØ³ Ú©ÙˆÙØªÛŒ Ø±Ùˆâ€Œ Ù†Ú¯ÛŒ...    SAD\n",
       "\n",
       "[4923 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_excel('./train_fa2.xlsx')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€Œ Ù‡Ù…ÛŒÙ†Ú©Ù‡ ØªÙˆÛŒ Ø§ÙˆØ¶Ø§Ø¹ ÙØ¹Ù„ÛŒØŒ ÙØ±Ø¯Ø§ Ú©Ù†Ú©ÙˆØ± Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¨Ù†Ø¸Ø±Ù… Ø®ÙˆØ¯Ø´ ÛŒÙ‡ Ø¨ÙØ±Ø¯ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒØ´Ù‡...\n",
      "HAPPY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 4923)\n",
    "print(df4.loc[i, 'Ú©ÛŒ Ú¯ÙØªÙ‡ Ù…Ø±Ø¯ Ú¯Ø±ÛŒÙ‡ Ù†Ù…ÛŒÚ©Ù†Ù‡!ØŸ!ØŸ Ø³ÛŒÙ„Ù… Ø§Ù…Ø´Ø¨ Ø³ÛŒÙ„ #Ø§ØµÙÙ‡Ø§Ù†'])\n",
    "print(df4.loc[i, 'SAD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ø­Ù„Ù‡ØŒ Ø­Ø§Ù„Ø§ Ú©Ù‡ Ø¯ÛŒØªØ§ Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØŒ Ø¨Ø±ÛŒÙ… Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ú†ÛŒÚ©Ø§Ø± Ø¨Ø§ÛŒØ¯ Ø¨Ú©Ù†ÛŒ. Ø¬ÙˆØ±ÛŒ ØªÙˆØ¶ÛŒØ­ Ù…ÛŒØ¯Ù… Ú©Ù‡ Ø§Ù†Ú¯Ø§Ø± Ø¯Ø§Ø±ÛŒ ÛŒÙ‡ Ø¨Ø§Ø²ÛŒ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯ÛŒ Ùˆ Ù‡Ø± Ù…Ø±Ø­Ù„Ù‡ Ø±Ùˆ Ø¨Ø§ÛŒØ¯ Ø¯Ø±Ø³Øª Ø±Ø¯ Ú©Ù†ÛŒ ØªØ§ Ø¨Ø±ÛŒ Ø¨Ø¹Ø¯ÛŒ.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª** (Ù…Ø«Ù„ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†Ù‚Ø´Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø²ÛŒ)  \n",
    "Ù‚Ø¨Ù„ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ø± Ú©Ø§Ø±ÛŒ Ø¨Ú©Ù†ÛŒØŒ Ø¨Ø§ÛŒØ¯ ÛŒÙ‡ Ù†Ú¯Ø§Ù‡ Ú©Ù„ÛŒ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø³ØªØª Ø¨Ù†Ø¯Ø§Ø²ÛŒ. Ø§ÛŒÙ† Ú©Ø§Ø± Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ú©Ù‡ Ø¨ÙÙ‡Ù…ÛŒ:  \n",
    "- **Ú†Ù†Ø¯ ØªØ§ Ø¯Ø§Ø¯Ù‡ Ø¯Ø§Ø±ÛŒØŸ** (Ø®Ø¨ Ú¯ÙØªÛŒ Û±Û³,Û°Û°Û° ØªØ§ØŒ Ø§ÙˆÚ©ÛŒ)  \n",
    "- **Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú†ÛŒ Ù‡Ø³ØªÙ†ØŸ** (Ù…Ø«Ù„Ø§Ù‹ØŒ Ø³ØªÙˆÙ† Ù…ØªÙ† ØªÙˆÛŒÛŒØªØŒ Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ...)  \n",
    "- **Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú†Ø·ÙˆØ±Ù‡ØŸ** (Ù…Ø«Ù„Ø§Ù‹ØŒ Ù†Ú©Ù†Ù‡ Û¹Û°Ùª ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ \"Ø®ÙˆØ´Ø­Ø§Ù„\" Ø¨Ø§Ø´Ù† Ùˆ ÙÙ‚Ø· Û±Û°Ùª Ø¨Ù‚ÛŒÙ‡ØŸ Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ù…ÛŒØ´Ù‡)  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # Ø§ÛŒÙ†Ùˆ Ø¨Ø§ Ø§Ø³Ù… ÙØ§ÛŒÙ„Øª Ø¹ÙˆØ¶ Ú©Ù†\n",
    "print(df.head())  # Ù†Ù…Ø§ÛŒØ´ Ûµ Ø³Ø·Ø± Ø§ÙˆÙ„\n",
    "print(df.info())  # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¯ÛŒØªØ§Ø³Øª\n",
    "print(df['label'].value_counts())  # Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "```\n",
    "âœ **Ø§Ú¯Ø± ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ oversampling ÛŒØ§ undersampling Ø±Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ (Ø¨Ø¹Ø¯Ø§Ù‹ Ø¨Ù‡Ø´ Ù…ÛŒâ€ŒØ±Ø³ÛŒÙ…).**  \n",
    "\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§ Ø¯Ø± Ù¾Ø§Ù†Ø¯Ø§Ø³](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” **Ù…Ø±Ø­Ù„Ù‡ Û²: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Data Cleaning)** (Ù…Ø«Ù„ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¨Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹)  \n",
    "Ø¨Ø§ÛŒØ¯ Ù…Ø·Ù…Ø¦Ù† Ø¨Ø´ÛŒ Ú©Ù‡ Ø¯ÛŒØªØ§ÛŒ Ø®Ø§Ù…Øª Ù…Ø´Ú©Ù„ Ø®Ø§ØµÛŒ Ù†Ø¯Ø§Ø±Ù‡. Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬ Ø§ÛŒÙ†Ø§ Ù‡Ø³ØªÙ†:  \n",
    "âœ… **Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ (Missing Values)**  \n",
    "âœ… **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ (Duplicate Data)**  \n",
    "âœ… **ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†ÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ø´Ø§Ù…Ù„ ÙÙ‚Ø· @ Ùˆ # Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ù…ØªÙ† Ù…ÙÛŒØ¯)**  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ:**  \n",
    "```python\n",
    "# Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ\n",
    "df = df.dropna()\n",
    "\n",
    "# Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± ØªÚ©Ø±Ø§Ø±ÛŒ\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ù„ÛŒÙ†Ú© Ùˆ Ù…Ù†Ø´Ù† Ù‡Ø³ØªÙ†\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø¢Ù…ÙˆØ²Ø´ Data Cleaning Ø¯Ø± Ù¾Ø§Ù†Ø¯Ø§Ø³](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›  **Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† (Text Preprocessing)** (Ù…Ø«Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ø§Ø±Ø§Ú©ØªØ± ØªÙˆÛŒ Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø§Ù„Ø§Ù† Ø¨Ø§ÛŒØ¯ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÛŒÛŒØª Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ø«ÛŒÙ Ù‡Ø³ØªÙ† Ùˆ Ù¾Ø± Ø§Ø² Ú†ÛŒØ²Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ù…Ø«Ù„:  \n",
    "âŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ (https://...)  \n",
    "âŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ (@user)  \n",
    "âŒ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ (#Ù…ÙˆØ¶ÙˆØ¹)  \n",
    "âŒ Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ (ğŸ˜‚ğŸ˜¡â¤ï¸)  \n",
    "âŒ Ø­Ø±ÙˆÙ Ø§Ø¶Ø§ÙÛŒ Ú©Ø´ÛŒØ¯Ù‡ (\"Ø¹Ø§Ø§Ø§Ø§Ù„ÛŒÛŒÛŒÛŒ\")  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # Ø­Ø°Ù Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # Ø­Ø°Ù Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š **Ù…Ø±Ø­Ù„Ù‡ Û´: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Exploratory Data Analysis - EDA)** (Ù…Ø«Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹)  \n",
    "Ø¨Ø§ÛŒØ¯ ÛŒÙ‡ Ú©Ù… Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Øª Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ Ú©Ù‡ Ø¨ÙÙ‡Ù…ÛŒ:  \n",
    "- **Ú©Ø¯ÙˆÙ… Ú©Ù„Ù…Ø§Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù†ØŸ**  \n",
    "- **Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø§Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ**  \n",
    "- **Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú†Ù‚Ø¯Ø± ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù†ØŸ**  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª Ù¾Ø± ØªÚ©Ø±Ø§Ø±:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[EDA Ø¯Ø± NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ **Ù…Ø±Ø­Ù„Ù‡ Ûµ: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¹Ø¯Ø¯ (Tokenization & Embedding)** (Ù…Ø«Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù† Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª)  \n",
    "Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ù…ÛŒâ€ŒÙÙ‡Ù…Ù†ØŒ Ù¾Ø³ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø±Ùˆ Ø¨Ù‡ Ø¹Ø¯Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…. Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡:  \n",
    "- **Bag of Words (BoW)** â€“ Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ú©Ù‡ ÙÙ‚Ø· ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ùˆ Ù…ÛŒâ€ŒØ´Ù…Ø±Ù‡  \n",
    "- **TF-IDF** â€“ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± Ù…ØªÙ†  \n",
    "- **Word Embeddings (Ù…Ø«Ù„ Word2Vec ÛŒØ§ BERT)** â€“ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ  \n",
    "\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² BERT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ù…Ù‚Ø¯Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø± Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ— **Ù…Ø±Ø­Ù„Ù‡ Û¶: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ (Training a Model)** (Ù…Ø«Ù„ Ø³Ø§Ø®ØªÙ† Ø´Ø®ØµÛŒØª Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø¨Ø§ÛŒØ¯ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±ÛŒ Ú©Ù‡ Ø§Ø² Ú†Ù‡ Ù…Ø¯Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:  \n",
    "âœ… **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ (Ù…Ø«Ù„ Naive Bayes, SVM)** â€“ Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø³Øª Ú©ÙˆÚ†ÛŒÚ© Ø¨Ø§Ø´Ù‡  \n",
    "âœ… **Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± (Ù…Ø«Ù„ LSTM, BERT)** â€“ Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ø´Ù‡ Ùˆ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ø¨Ø®ÙˆØ§ÛŒ  \n",
    "\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "ğŸ“Œ **Ø§Ú¯Ø± Ø§Ø² BERT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ BERT Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Ù…Ø±Ø­Ù„Ù‡ Û·: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ (Evaluation & Testing)** (Ù…Ø«Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ø²ÛŒ)  \n",
    "Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ØŒ Ø¨Ø§ÛŒØ¯ Ø¨Ø¨ÛŒÙ†ÛŒ Ú†Ù‚Ø¯Ø± Ø®ÙˆØ¨ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ù‡. Ø§Ø² Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ **Ø¯Ù‚Øª (Accuracy)ØŒ Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision)ØŒ ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall)** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.  \n",
    "\n",
    "ğŸ“Œ **Ú©Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "ğŸ“Œ **Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**  \n",
    "[Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Ø­Ø§Ù„Ø§ ØªÙˆ Ø¨Ú¯ÙˆØŒ ØªÙˆ Ú©Ø¯ÙˆÙ… Ù…Ø±Ø­Ù„Ù‡ Ø³ÙˆØ§Ù„ Ø¯Ø§Ø±ÛŒØŸ** ğŸ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯ÙˆØªØ§ Ø¯ÙˆØ¯ Ù…ÛŒØ²Ù†ÛŒ Ù‡Ù…ÙˆÙ† Ø®Ø§ØµÛŒØª Ù‚Ø±ØµØ§ÛŒ Ø®Ù†Ø¯Ø§Ù† Ø±Ùˆ Ø¯Ø§Ø±Ù‡\n",
      "#Ù†Ù‡_Ø¨Ù‡_Ø¬Ù…Ù‡ÙˆØ±ÛŒ_Ø§Ø³Ù„Ø§Ù…ÛŒ\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø´Ø§Ø¯ Ø¨Ø§ \"happy\" ÛŒØ§ \"joyful\" Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒØ´Ù†\n",
    "happy_tweets = df[df['emotion'].isin(['joy'])]\n",
    "\n",
    "# Ø§Ù†ØªØ®Ø§Ø¨ ÛŒÚ© Ø±Ø¯ÛŒÙ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø¯\n",
    "i = random.randint(0, len(happy_tweets) - 1)\n",
    "print(happy_tweets.iloc[i]['tweet'])\n",
    "print(happy_tweets.iloc[i]['emotion'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i= random.randint(0, 113829)\n",
    "# print(df.loc[i, 'tweet'])\n",
    "# print(df.loc[i, 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113829 entries, 0 to 113828\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   tweet         113829 non-null  object\n",
      " 1   replyCount    113829 non-null  int64 \n",
      " 2   retweetCount  113829 non-null  int64 \n",
      " 3   likeCount     113829 non-null  int64 \n",
      " 4   quoteCount    113829 non-null  int64 \n",
      " 5   hashtags      113829 non-null  object\n",
      " 6   sourceLabel   113829 non-null  object\n",
      " 7   emotion       113829 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n",
      "emotion\n",
      "sad         30.157517\n",
      "joy         24.619385\n",
      "anger       17.630832\n",
      "fear        15.482873\n",
      "surprise    11.296770\n",
      "disgust      0.812622\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of labels\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "print(df['emotion'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Repetitive Rows\n",
    "#df= df.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
