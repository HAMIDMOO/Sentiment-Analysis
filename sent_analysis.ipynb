{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Twitter Dataset\n",
    "About Dataset\n",
    "\n",
    "This dataset contains more than 3300 Persian tweets, crawled from Twitter.com\n",
    "Each tweet is assigned a label, which is a number between 0 to 4.\n",
    "Label 0 indicates the sentiment of Happiness and Joy.\n",
    "Label 1 indicates the sentiment of Sadness.\n",
    "Label 2 indicates the sentiment of Anger and Furiosity.\n",
    "Label 3 indicates the sentiment of Neutral.\n",
    "And finally, label 4 indicates the sentiment of intense emotions, such as Surprise, Fear, and Love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Numeric Labels</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>احساس می‌کنم غایت زندگی من اینه که یکی یروز بف...</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ای بابا نصیب هرکسی نمیشه که، فقط دعای خیر پدر ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>با من مثل بقیه رفتار نکن آشغال</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>من علاقه شدیدی به شنیدن «بغلم کن» از طرف اونی ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>چیز کیک گرم و نرم میخوام.</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>مملکت نیست که تونل وحشته. بچه رو میبرن قاتل با...</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>قرار بود امتحانا که تموم شد مثلا یکم کصکلک کنم...</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>دوستان اون سه نقطه قرمز ها هست که میزنی باز می...</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>نتیجه ریسرچ کم‌کم داره به سمتی خوبی پیش میره</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>3380</td>\n",
       "      <td>سلام بچه‌ها. چه خبرا؟</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3381 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             Tweets  \\\n",
       "0              0  احساس می‌کنم غایت زندگی من اینه که یکی یروز بف...   \n",
       "1              1  ای بابا نصیب هرکسی نمیشه که، فقط دعای خیر پدر ...   \n",
       "2              2                     با من مثل بقیه رفتار نکن آشغال   \n",
       "3              3  من علاقه شدیدی به شنیدن «بغلم کن» از طرف اونی ...   \n",
       "4              4                          چیز کیک گرم و نرم میخوام.   \n",
       "...          ...                                                ...   \n",
       "3376        3376  مملکت نیست که تونل وحشته. بچه رو میبرن قاتل با...   \n",
       "3377        3377  قرار بود امتحانا که تموم شد مثلا یکم کصکلک کنم...   \n",
       "3378        3378  دوستان اون سه نقطه قرمز ها هست که میزنی باز می...   \n",
       "3379        3379      نتیجه ریسرچ کم‌کم داره به سمتی خوبی پیش میره    \n",
       "3380        3380                              سلام بچه‌ها. چه خبرا؟   \n",
       "\n",
       "      Numeric Labels             Label  \n",
       "0                  4  Intense Emotions  \n",
       "1                  0             Happy  \n",
       "2                  2             Angry  \n",
       "3                  4  Intense Emotions  \n",
       "4                  0             Happy  \n",
       "...              ...               ...  \n",
       "3376               2             Angry  \n",
       "3377               2             Angry  \n",
       "3378               0             Happy  \n",
       "3379               0             Happy  \n",
       "3380               3           Neutral  \n",
       "\n",
       "[3381 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2= pd.read_csv('./PersianTwitterDataset.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این حیدر حیدر چقدر میکس زیبا و دقیقیه🫶\n",
      "Intense Emotions\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 3300)\n",
    "print(df2.loc[i, 'Tweets'])\n",
    "print(df2.loc[i, 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian tweets emotional dataset\n",
    "About Dataset\n",
    "New Persian Dataset\n",
    "Since Persian datasets are really scarce I scrape Twitter in order to make a new Persian dataset.\n",
    "\n",
    "The tweets have been pulled from Twitter using snscrape and manual tagging has been done based on Ekman's 6 main emotions.\n",
    "For privacy sake, I pre-process and remove usernames, display names, and mentions from all tweets. Also, I deleted the timestamps and Tweets IDs.\n",
    "\n",
    "Columns:\n",
    "1) tweet\n",
    "2) replyCount\n",
    "3) retweetCount\n",
    "4) likeCount\n",
    "5) quoteCount\n",
    "6) hashtags\n",
    "7) sourceLabel\n",
    "8) emotion\n",
    "\n",
    "Please leave an upvote if you find this relevant. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "sad         34328\n",
       "joy         28024\n",
       "anger       20069\n",
       "fear        17624\n",
       "surprise    12859\n",
       "disgust       925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('./merged.csv')\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چون همش با دوربین ثبت شده ، ایا میشه اعتراض زد...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اين وضع ب طرز خنده داري گريه داره ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خب من رسما از یک نفر متنفرم،چون از گربه بدش می...</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>مرحوم پیش بینی آبکی زیاد میکرد     مرحوم عجب آ...</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>کلا عین اعتقادات و توئیت زدناتون ... !!   در ق...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>خب وقتی میگی کسی بیاد مارو بگیره یارو ترس میکن...</td>\n",
       "      <td>FEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>همون هارو     مگه آهنگ جدیدای خواننده‌های دهه ...</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>نیم دگیرش چطور حل نیشد</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet   emotion\n",
       "0        از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم      HATE\n",
       "1      \"کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم...       SAD\n",
       "2     چون همش با دوربین ثبت شده ، ایا میشه اعتراض زد...     OTHER\n",
       "3                 اين وضع ب طرز خنده داري گريه داره ...       SAD\n",
       "4     خب من رسما از یک نفر متنفرم،چون از گربه بدش می...      HATE\n",
       "...                                                 ...       ...\n",
       "6151  مرحوم پیش بینی آبکی زیاد میکرد     مرحوم عجب آ...  SURPRISE\n",
       "6152  کلا عین اعتقادات و توئیت زدناتون ... !!   در ق...     ANGRY\n",
       "6153  خب وقتی میگی کسی بیاد مارو بگیره یارو ترس میکن...      FEAR\n",
       "6154  همون هارو     مگه آهنگ جدیدای خواننده‌های دهه ...  SURPRISE\n",
       "6155                            نیم دگیرش چطور حل نیشد      OTHER\n",
       "\n",
       "[6156 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_excel('./train_fa.xlsx')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamid\\AppData\\Local\\Temp\\ipykernel_31384\\2986185299.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df3 = df3[~df['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# drop the rows tha have nan values\n",
    "df3= df3.dropna()\n",
    "print(df3.isna().sum())\n",
    "\n",
    "# drop the duplicated values\n",
    "df3= df3.drop_duplicates()\n",
    "print(df3.duplicated().sum())\n",
    "df3\n",
    "\n",
    "# Filter rows that only contain links, @ or #\n",
    "df3 = df3[~df['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # حذف لینک‌ها\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # حذف منشن‌ها (@mentions)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # حذف هشتگ‌ها (#hashtags)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # حذف علائم نگارشی\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # حذف فاصله‌های اضافی\n",
    "    return text\n",
    "\n",
    "df3['clean_text'] = df3['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: با یکی از بچهها که شانس آوردم خواب مونده بود و هنوز نرفته بود ۱۳ به در رفتیم مرکز داده با وجودی که مجوز تردد داشتیم حراست راهمون نمیداد با کلی بدبختی و تماس هماهنگ کردیم رفتیم داخل پای سرور ۲۵\n",
      "Processed Text: ['بچه', 'شانس', 'آوردم', 'خواب', 'مونده_بود', 'نرفته_بود', 'رفتیم', 'مرکز', 'وجودی', 'مجوز', 'تردد', 'داشت', 'حراست', 'راهمون', 'نمیداد', 'بدبختی', 'تماس', 'هماهنگ', 'کردیم', 'رفتیم', 'داخل', 'پای', 'سرور']\n"
     ]
    }
   ],
   "source": [
    "from hazm import Normalizer, WordTokenizer, stopwords_list, Lemmatizer\n",
    "import random\n",
    "import string\n",
    "\n",
    "def preprocessing(text, Apply_normalizer= True, Aplly_wordtokenizer= True, remove_stop_words= True, Apply_lemmatizer=True, remove_numbers=True):\n",
    "  '''\n",
    "  Cleaning and preprocessing the given text.\n",
    "\n",
    "  Args:\n",
    "    text (str): the input text that we want to work on in.\n",
    "    Apply_normalizer (bool): if True, shows that text will bee normalize by hazm. Defualt is True\n",
    "    Aplly_wordtokenizer (bool): if True, the text will tokenize. Defualt is True\n",
    "    remove_stop_words (list): a list that have the set of stopwords in pesian. Defualt is True\n",
    "    Apply_lemmatizer (bool): change the word to its root. Defualt is True\n",
    "    remove_numbers (bool): remove the numbers from text\n",
    "    replace the \\u200c:( text = \"می‌خوانم کتابی از کتاب‌خانه و دانش‌آموزان را می‌بینم.\"\n",
    "                        processed_text = ['می\\u200cخوانم', 'کتابی', 'از', 'کتاب\\u200cخانه', 'و', 'دانش\\u200cآموزان', 'را', 'می\\u200cبینم'])\n",
    "\n",
    "\n",
    "  Return:\n",
    "    filtered (list): list of words(of text) after cleaning and preprocessing.\n",
    "  '''\n",
    "\n",
    "  \n",
    "  # Initinalize requiared tools from Hazm\n",
    "  normalizer= Normalizer()\n",
    "  wordtokenizer= WordTokenizer()\n",
    "  stopword_lst= stopwords_list()\n",
    "  lemmatizer = Lemmatizer()\n",
    "  panctuation= string.punctuation +  \"؟،؛.,#\"\n",
    "  \n",
    "\n",
    "\n",
    "  # Step 1 : Normalize the text\n",
    "  if Apply_normalizer:\n",
    "    text= normalizer.normalize(text)\n",
    "\n",
    "\n",
    "    \n",
    "  # Step 2: Tokenize the text to words\n",
    "  if Aplly_wordtokenizer:\n",
    "    words= wordtokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "  # Step 3: delete the aditional numbers and words\n",
    "  if remove_numbers:\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "\n",
    "\n",
    "  # step 4 : Delet the stop words from  words(list)\n",
    "  if remove_stop_words:\n",
    "    words= [word for word in words if word not in stopword_lst]\n",
    "\n",
    "\n",
    "\n",
    "  # Step 5: delete the additionala (.?!,)\n",
    "  if panctuation:\n",
    "    words= [word for word in words if word not in panctuation]  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  #step 6: find the root of words\n",
    "  if Apply_lemmatizer:\n",
    "    words= [lemmatizer.lemmatize(word, pos= 'v') for word in words]\n",
    "\n",
    "\n",
    "  #ُStep 7: replace the '\\u200c'  \n",
    "  filtered= [word.replace('\\u200c', '')for word in words]\n",
    "\n",
    "  return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Kodun ne yaptığını görmek için 😊\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x= random.randint(1, 6000)\n",
    "    sample_text = df3['clean_text'][x]\n",
    "    processed_text = preprocessing(\n",
    "        sample_text)\n",
    "    print(\"Original Text:\", sample_text)\n",
    "    print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessing)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[65], line 26\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(text, Apply_normalizer, Aplly_wordtokenizer, remove_stop_words, Apply_lemmatizer, remove_numbers)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mCleaning and preprocessing the given text.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m  filtered (list): list of words(of text) after cleaning and preprocessing.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Initinalize requiared tools from Hazm\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m normalizer\u001b[38;5;241m=\u001b[39m Normalizer()\n\u001b[0;32m     27\u001b[0m wordtokenizer\u001b[38;5;241m=\u001b[39m WordTokenizer()\n\u001b[0;32m     28\u001b[0m stopword_lst\u001b[38;5;241m=\u001b[39m stopwords_list()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\normalizer.py:171\u001b[0m, in \u001b[0;36mNormalizer.__init__\u001b[1;34m(self, correct_spacing, remove_diacritics, remove_specials_chars, decrease_repeated_chars, persian_style, persian_numbers, unicodes_replacement, seperate_mi)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecials_chars_patterns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;66;03m# Remove almoast all arabic unicode superscript and subscript characters in the ranges of 00600-06FF, 08A0-08FF, FB50-FDFF, and FE70-FEFF\u001b[39;00m\n\u001b[0;32m    164\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         ),\n\u001b[0;32m    168\u001b[0m     ]\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seperate_mi:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbs \u001b[38;5;241m=\u001b[39m Lemmatizer(joined_verb_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mverbs\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_mi_patterns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbن?می[آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی]+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unicodes_replacement:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\lemmatizer.py:49\u001b[0m, in \u001b[0;36mLemmatizer.__init__\u001b[1;34m(self, words_file, verbs_file, joined_verb_parts)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer \u001b[38;5;241m=\u001b[39m Stemmer()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconjugation \u001b[38;5;241m=\u001b[39m Conjugation()\n\u001b[1;32m---> 49\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WordTokenizer(words_file\u001b[38;5;241m=\u001b[39mdefault_words, verbs_file\u001b[38;5;241m=\u001b[39mverbs_file)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbs_file:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\word_tokenizer.py:103\u001b[0m, in \u001b[0;36mWordTokenizer.__init__\u001b[1;34m(self, words_file, verbs_file, join_verb_parts, join_abbreviations, separate_emoji, replace_links, replace_ids, replace_emails, replace_numbers, replace_hashtags)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# NOTE: python2.7 does not support unicodes with \\w\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhashtag_repl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAG \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;241m0\u001b[39m]: (item[\u001b[38;5;241m1\u001b[39m], item[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m words_list(words_file)}\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join_verb_parts:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_verbs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mام\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mای\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mنخواهند_شد\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\utils.py:48\u001b[0m, in \u001b[0;36mwords_list\u001b[1;34m(words_file)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[0;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     49\u001b[0m         (item[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hazm\\utils.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[0;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     49\u001b[0m         (item[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df3[\"clean_text\"] = df3[\"clean_text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عکسی که چند روز پیش گذاشته بودم این فیلم الانش...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>این خاک مال مردمان است نه حاکمان #ایران #مهسا_...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اگه تو بغلت بودم حالم خیلی بهتر میشد</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>گاز نداریم آب نداریم برق نداریم نت نداریم پول ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان    SAD\n",
       "0     عکسی که چند روز پیش گذاشته بودم این فیلم الانش...  OTHER\n",
       "1     تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...    SAD\n",
       "2              خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه  HAPPY\n",
       "3     این خاک مال مردمان است نه حاکمان #ایران #مهسا_...  ANGRY\n",
       "4                  اگه تو بغلت بودم حالم خیلی بهتر میشد    SAD\n",
       "...                                                 ...    ...\n",
       "4918  من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه  HAPPY\n",
       "4919  گاز نداریم آب نداریم برق نداریم نت نداریم پول ...    SAD\n",
       "4920  یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...    SAD\n",
       "4921  زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...  OTHER\n",
       "4922  سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...    SAD\n",
       "\n",
       "[4923 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_excel('./train_fa2.xlsx')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‌ همینکه توی اوضاع فعلی، فردا کنکور نداریم بنظرم خودش یه بُرد محسوب میشه...\n",
      "HAPPY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 4923)\n",
    "print(df4.loc[i, 'کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان'])\n",
    "print(df4.loc[i, 'SAD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حله، حالا که دیتا رو پیدا کردی، بریم مرحله به مرحله ببینیم چیکار باید بکنی. جوری توضیح میدم که انگار داری یه بازی مرحله‌ای انجام میدی و هر مرحله رو باید درست رد کنی تا بری بعدی.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **مرحله ۱: بررسی اولیه دیتاست** (مثل نگاه کردن به نقشه قبل از شروع بازی)  \n",
    "قبل از اینکه هر کاری بکنی، باید یه نگاه کلی به دیتاستت بندازی. این کار کمک می‌کنه که بفهمی:  \n",
    "- **چند تا داده داری؟** (خب گفتی ۱۳,۰۰۰ تا، اوکی)  \n",
    "- **ستون‌های دیتاست چی هستن؟** (مثلاً، ستون متن توییت، ستون برچسب احساسات و ...)  \n",
    "- **مقدار داده‌ها در هر کلاس احساسات چطوره؟** (مثلاً، نکنه ۹۰٪ توییت‌ها \"خوشحال\" باشن و فقط ۱۰٪ بقیه؟ این باعث عدم تعادل میشه)  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی اولیه:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # اینو با اسم فایلت عوض کن\n",
    "print(df.head())  # نمایش ۵ سطر اول\n",
    "print(df.info())  # نمایش اطلاعات کلی درباره دیتاست\n",
    "print(df['label'].value_counts())  # بررسی توزیع برچسب‌ها\n",
    "```\n",
    "✍ **اگر توزیع داده‌ها نامتعادل بود، باید روش‌هایی مثل oversampling یا undersampling رو انجام بدی (بعداً بهش می‌رسیم).**  \n",
    "\n",
    "📌 **منبع پیشنهادی برای بررسی اولیه دیتاست:**  \n",
    "[راهنمای بررسی اولیه دیتا در پانداس](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 **مرحله ۲: پاکسازی داده‌ها (Data Cleaning)** (مثل تمیز کردن صفحه بازی قبل از شروع)  \n",
    "باید مطمئن بشی که دیتای خامت مشکل خاصی نداره. مشکلات رایج اینا هستن:  \n",
    "✅ **مقادیر خالی (Missing Values)**  \n",
    "✅ **داده‌های تکراری (Duplicate Data)**  \n",
    "✅ **توییت‌های بی‌معنی (مثلاً شامل فقط @ و # و لینک‌ها بدون متن مفید)**  \n",
    "\n",
    "📌 **کد پیشنهادی برای پاکسازی:**  \n",
    "```python\n",
    "# حذف مقادیر خالی\n",
    "df = df.dropna()\n",
    "\n",
    "# حذف مقادیر تکراری\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# بررسی نمونه‌هایی که فقط شامل لینک و منشن هستن\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[آموزش Data Cleaning در پانداس](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 **مرحله ۳: پیش‌پردازش متن (Text Preprocessing)** (مثل تنظیمات اولیه کاراکتر توی بازی)  \n",
    "الان باید متن‌های توییت رو برای مدل آماده کنیم. توییت‌ها معمولاً کثیف هستن و پر از چیزهای اضافی مثل:  \n",
    "❌ لینک‌ها (https://...)  \n",
    "❌ نام‌های کاربری (@user)  \n",
    "❌ هشتگ‌ها (#موضوع)  \n",
    "❌ ایموجی‌ها (😂😡❤️)  \n",
    "❌ حروف اضافی کشیده (\"عاااالیییی\")  \n",
    "\n",
    "📌 **کد پیشنهادی برای تمیز کردن متن:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # حذف لینک‌ها\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # حذف منشن‌ها\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # حذف هشتگ‌ها\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # حذف علائم نگارشی\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # حذف فاصله‌های اضافی\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای پیش‌پردازش متن](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **مرحله ۴: تحلیل داده‌ها (Exploratory Data Analysis - EDA)** (مثل بررسی ویژگی‌های بازی قبل از شروع)  \n",
    "باید یه کم روی دیتات آنالیز انجام بدی که بفهمی:  \n",
    "- **کدوم کلمات بیشتر استفاده شدن؟**  \n",
    "- **طول جملات چقدره؟**  \n",
    "- **احساسات چقدر توزیع شدن؟**  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی کلمات پر تکرار:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[EDA در NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 **مرحله ۵: تبدیل متن به عدد (Tokenization & Embedding)** (مثل آماده کردن کاراکترهای بازی برای حرکت)  \n",
    "مدل‌های یادگیری ماشین فقط عدد می‌فهمن، پس باید متن رو به عدد تبدیل کنیم. روش‌های مختلفی وجود داره:  \n",
    "- **Bag of Words (BoW)** – مدل ساده‌ای که فقط تعداد کلمات رو می‌شمره  \n",
    "- **TF-IDF** – مقدار اهمیت هر کلمه در متن  \n",
    "- **Word Embeddings (مثل Word2Vec یا BERT)** – تبدیل کلمات به بردار عددی  \n",
    "\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[مقدمه‌ای بر Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏗 **مرحله ۶: مدل‌سازی (Training a Model)** (مثل ساختن شخصیت بازی)  \n",
    "باید تصمیم بگیری که از چه مدلی استفاده کنی:  \n",
    "✅ **مدل‌های ساده (مثل Naive Bayes, SVM)** – اگر دیتاست کوچیک باشه  \n",
    "✅ **مدل‌های پیچیده‌تر (مثل LSTM, BERT)** – اگر دیتاست بزرگ باشه و دقت بالا بخوای  \n",
    "\n",
    "📌 **اگر از مدل ساده استفاده کنی:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "[راهنمای آموزش مدل BERT برای تحلیل احساسات](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **مرحله ۷: ارزیابی مدل (Evaluation & Testing)** (مثل بررسی امتیاز بازی)  \n",
    "بعد از آموزش مدل، باید ببینی چقدر خوب کار میکنه. از معیارهای **دقت (Accuracy)، دقت مثبت (Precision)، یادآوری (Recall)** استفاده کن.  \n",
    "\n",
    "📌 **کد ارزیابی مدل:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای ارزیابی مدل‌های NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **حالا تو بگو، تو کدوم مرحله سوال داری؟** 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " دوتا دود میزنی همون خاصیت قرصای خندان رو داره\n",
      "#نه_به_جمهوری_اسلامی\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# فرض می‌کنیم که احساسات شاد با \"happy\" یا \"joyful\" مشخص می‌شن\n",
    "happy_tweets = df[df['emotion'].isin(['joy'])]\n",
    "\n",
    "# انتخاب یک ردیف تصادفی از داده‌های شاد\n",
    "i = random.randint(0, len(happy_tweets) - 1)\n",
    "print(happy_tweets.iloc[i]['tweet'])\n",
    "print(happy_tweets.iloc[i]['emotion'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i= random.randint(0, 113829)\n",
    "# print(df.loc[i, 'tweet'])\n",
    "# print(df.loc[i, 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113829 entries, 0 to 113828\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   tweet         113829 non-null  object\n",
      " 1   replyCount    113829 non-null  int64 \n",
      " 2   retweetCount  113829 non-null  int64 \n",
      " 3   likeCount     113829 non-null  int64 \n",
      " 4   quoteCount    113829 non-null  int64 \n",
      " 5   hashtags      113829 non-null  object\n",
      " 6   sourceLabel   113829 non-null  object\n",
      " 7   emotion       113829 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n",
      "emotion\n",
      "sad         30.157517\n",
      "joy         24.619385\n",
      "anger       17.630832\n",
      "fear        15.482873\n",
      "surprise    11.296770\n",
      "disgust      0.812622\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of labels\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "print(df['emotion'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Repetitive Rows\n",
    "#df= df.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
