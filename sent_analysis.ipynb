{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Twitter Dataset\n",
    "About Dataset\n",
    "\n",
    "This dataset contains more than 3300 Persian tweets, crawled from Twitter.com\n",
    "Each tweet is assigned a label, which is a number between 0 to 4.\n",
    "Label 0 indicates the sentiment of Happiness and Joy.\n",
    "Label 1 indicates the sentiment of Sadness.\n",
    "Label 2 indicates the sentiment of Anger and Furiosity.\n",
    "Label 3 indicates the sentiment of Neutral.\n",
    "And finally, label 4 indicates the sentiment of intense emotions, such as Surprise, Fear, and Love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Numeric Labels</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>احساس می‌کنم غایت زندگی من اینه که یکی یروز بفهمه دارم راجع به چی حرف می‌زنم</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ای بابا نصیب هرکسی نمیشه که، فقط دعای خیر پدر مادر :))))))))</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>با من مثل بقیه رفتار نکن آشغال</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>من علاقه شدیدی به شنیدن «بغلم کن» از طرف اونی که «می‌خوام بغلش کنم» دارم.</td>\n",
       "      <td>4</td>\n",
       "      <td>Intense Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>چیز کیک گرم و نرم میخوام.</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>مملکت نیست که تونل وحشته. بچه رو میبرن قاتل باباشو ببینه، اون یکی سر زنشو میبره. بابا قراره بخوابیما اینجا هم.</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>قرار بود امتحانا که تموم شد مثلا یکم کصکلک کنم که مریضی زد همشو نابود کرد. اههههههه</td>\n",
       "      <td>2</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>دوستان اون سه نقطه قرمز ها هست که میزنی باز میشه تا همه ی استادایی که ارائه میدن رو ببینی؛ من بعد از انتخاب واحد فهمیدم اونا چیه. و از انتخاب واحدم هم راضی ام. شب خوش🖐️</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>نتیجه ریسرچ کم‌کم داره به سمتی خوبی پیش میره</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>3380</td>\n",
       "      <td>سلام بچه‌ها. چه خبرا؟</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3381 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  \\\n",
       "0              0   \n",
       "1              1   \n",
       "2              2   \n",
       "3              3   \n",
       "4              4   \n",
       "...          ...   \n",
       "3376        3376   \n",
       "3377        3377   \n",
       "3378        3378   \n",
       "3379        3379   \n",
       "3380        3380   \n",
       "\n",
       "                                                                                                                                                                        Tweets  \\\n",
       "0                                                                                                 احساس می‌کنم غایت زندگی من اینه که یکی یروز بفهمه دارم راجع به چی حرف می‌زنم   \n",
       "1                                                                                                                 ای بابا نصیب هرکسی نمیشه که، فقط دعای خیر پدر مادر :))))))))   \n",
       "2                                                                                                                                               با من مثل بقیه رفتار نکن آشغال   \n",
       "3                                                                                                    من علاقه شدیدی به شنیدن «بغلم کن» از طرف اونی که «می‌خوام بغلش کنم» دارم.   \n",
       "4                                                                                                                                                    چیز کیک گرم و نرم میخوام.   \n",
       "...                                                                                                                                                                        ...   \n",
       "3376                                                            مملکت نیست که تونل وحشته. بچه رو میبرن قاتل باباشو ببینه، اون یکی سر زنشو میبره. بابا قراره بخوابیما اینجا هم.   \n",
       "3377                                                                                       قرار بود امتحانا که تموم شد مثلا یکم کصکلک کنم که مریضی زد همشو نابود کرد. اههههههه   \n",
       "3378  دوستان اون سه نقطه قرمز ها هست که میزنی باز میشه تا همه ی استادایی که ارائه میدن رو ببینی؛ من بعد از انتخاب واحد فهمیدم اونا چیه. و از انتخاب واحدم هم راضی ام. شب خوش🖐️   \n",
       "3379                                                                                                                             نتیجه ریسرچ کم‌کم داره به سمتی خوبی پیش میره    \n",
       "3380                                                                                                                                                     سلام بچه‌ها. چه خبرا؟   \n",
       "\n",
       "      Numeric Labels             Label  \n",
       "0                  4  Intense Emotions  \n",
       "1                  0             Happy  \n",
       "2                  2             Angry  \n",
       "3                  4  Intense Emotions  \n",
       "4                  0             Happy  \n",
       "...              ...               ...  \n",
       "3376               2             Angry  \n",
       "3377               2             Angry  \n",
       "3378               0             Happy  \n",
       "3379               0             Happy  \n",
       "3380               3           Neutral  \n",
       "\n",
       "[3381 rows x 4 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2= pd.read_csv('./PersianTwitterDataset.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حیف که دیگه اینا کنار هم نیستن انتقام امشب رو بگیرن، هرچند هشتا گل دیگه واقعا قفله 😂😂😂\n",
      "Happy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 3300)\n",
    "print(df2.loc[i, 'Tweets'])\n",
    "print(df2.loc[i, 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian tweets emotional dataset\n",
    "About Dataset\n",
    "New Persian Dataset\n",
    "Since Persian datasets are really scarce I scrape Twitter in order to make a new Persian dataset.\n",
    "\n",
    "The tweets have been pulled from Twitter using snscrape and manual tagging has been done based on Ekman's 6 main emotions.\n",
    "For privacy sake, I pre-process and remove usernames, display names, and mentions from all tweets. Also, I deleted the timestamps and Tweets IDs.\n",
    "\n",
    "Columns:\n",
    "1) tweet\n",
    "2) replyCount\n",
    "3) retweetCount\n",
    "4) likeCount\n",
    "5) quoteCount\n",
    "6) hashtags\n",
    "7) sourceLabel\n",
    "8) emotion\n",
    "\n",
    "Please leave an upvote if you find this relevant. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "sad         34328\n",
       "joy         28024\n",
       "anger       20069\n",
       "fear        17624\n",
       "surprise    12859\n",
       "disgust       925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('./merged.csv')\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6156 entries, 0 to 6155\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   tweet    6151 non-null   object\n",
      " 1   emotion  6154 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 96.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_excel('./train_fa.xlsx')\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet      5\n",
       "emotion    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم بعد از حدود 2 ماه استفاده«متأسفانه باخبر شدیم» که فردی در ایرانشهر به حداقل 41 دختر تجاوز کرده. امیدواریم با همکاری نمایندگان محترم مجلس و دستگاه قضا، دیگه به این راحتی باخبر نشیم.</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چون همش با دوربین ثبت شده ، ایا میشه اعتراض زد؟؟ و اصن تاثیر داره؟ کسی اگه اطلاعی داره ممنون میشم راهنمایی کنید</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اين وضع ب طرز خنده داري گريه داره ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خب من رسما از یک نفر متنفرم،چون از گربه بدش میاد از صبح شروع کرده رو مخ من راه رفتن؛شپش میگیری،کزاز میگیری،کک داره ،درد داره،مرض داره و اینقدر تکرار کرده که تا سرم میخاره میپرم جلوی آینه نگاه میکنم یه وقت شپش نگرفته باشم... عوضی ازش متنفرم بهش گفته بودم منو رو یه چیزی حساس نکن</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>مرحوم پیش بینی آبکی زیاد میکرد     مرحوم عجب آینده نگری و پیش بینی هایی داشت</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>کلا عین اعتقادات و توئیت زدناتون ... !!   در قبال رانت و نون مفت سفره انقلاب ...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>خب وقتی میگی کسی بیاد مارو بگیره یارو ترس میکنه  یکم دوست باشید تا اون حاله‌ای از نور رو ببینه در شما.   اونوقت اون طرف کنه میشه واسه ازدواج</td>\n",
       "      <td>FEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>همون هارو     مگه آهنگ جدیدای خواننده‌های دهه پنجاه رو گوش میدید ؟</td>\n",
       "      <td>SURPRISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>نیم دگیرش چطور حل نیشد</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                      tweet  \\\n",
       "0                                                                                                                                                                                                                                            از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم   \n",
       "1                                                        \"کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم بعد از حدود 2 ماه استفاده«متأسفانه باخبر شدیم» که فردی در ایرانشهر به حداقل 41 دختر تجاوز کرده. امیدواریم با همکاری نمایندگان محترم مجلس و دستگاه قضا، دیگه به این راحتی باخبر نشیم.   \n",
       "2                                                                                                                                                                           چون همش با دوربین ثبت شده ، ایا میشه اعتراض زد؟؟ و اصن تاثیر داره؟ کسی اگه اطلاعی داره ممنون میشم راهنمایی کنید   \n",
       "3                                                                                                                                                                                                                                                     اين وضع ب طرز خنده داري گريه داره ...   \n",
       "4     خب من رسما از یک نفر متنفرم،چون از گربه بدش میاد از صبح شروع کرده رو مخ من راه رفتن؛شپش میگیری،کزاز میگیری،کک داره ،درد داره،مرض داره و اینقدر تکرار کرده که تا سرم میخاره میپرم جلوی آینه نگاه میکنم یه وقت شپش نگرفته باشم... عوضی ازش متنفرم بهش گفته بودم منو رو یه چیزی حساس نکن   \n",
       "...                                                                                                                                                                                                                                                                                     ...   \n",
       "6151                                                                                                                                                                                                         مرحوم پیش بینی آبکی زیاد میکرد     مرحوم عجب آینده نگری و پیش بینی هایی داشت     \n",
       "6152                                                                                                                                                                                                       کلا عین اعتقادات و توئیت زدناتون ... !!   در قبال رانت و نون مفت سفره انقلاب ...   \n",
       "6153                                                                                                                                          خب وقتی میگی کسی بیاد مارو بگیره یارو ترس میکنه  یکم دوست باشید تا اون حاله‌ای از نور رو ببینه در شما.   اونوقت اون طرف کنه میشه واسه ازدواج    \n",
       "6154                                                                                                                                                                                                                     همون هارو     مگه آهنگ جدیدای خواننده‌های دهه پنجاه رو گوش میدید ؟   \n",
       "6155                                                                                                                                                                                                                                                                نیم دگیرش چطور حل نیشد    \n",
       "\n",
       "       emotion  \n",
       "0         HATE  \n",
       "1          SAD  \n",
       "2        OTHER  \n",
       "3          SAD  \n",
       "4         HATE  \n",
       "...        ...  \n",
       "6151  SURPRISE  \n",
       "6152     ANGRY  \n",
       "6153      FEAR  \n",
       "6154  SURPRISE  \n",
       "6155     OTHER  \n",
       "\n",
       "[6112 rows x 2 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# drop the rows tha have nan values\n",
    "df3= df3.dropna()\n",
    "print(df3.isna().sum())\n",
    "\n",
    "# drop the duplicated values\n",
    "df3= df3.drop_duplicates()\n",
    "print(df3.duplicated().sum())\n",
    "df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # حذف لینک‌ها\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # حذف منشن‌ها (@mentions)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # حذف هشتگ‌ها (#hashtags)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # حذف علائم نگارشی\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # حذف فاصله‌های اضافی\n",
    "    return text\n",
    "\n",
    "df3['clean_text'] = df3['tweet'].apply(clean_text)\n",
    "\n",
    "\n",
    "# i = df3.apply(lambda row: 1 if row['clean_text'] != row['tweet'] else 0, axis=1).sum()\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_to_remove = df3[df3['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "# # چاپ سطر‌هایی که حذف می‌شوند\n",
    "# print(\"سطر‌های حذف شده:\")\n",
    "# print(len(rows_to_remove))\n",
    "\n",
    "\n",
    "# # Filter rows that only contain links, @ or #\n",
    "# df3 = df3[~df3['tweet'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: دو صبح متوالی راننده تاکسی اعصابم جویید که چرا پنج تومانی دارید و خرد ندارید این تخلیه و غر زدن روزی تمامی دارد\n",
      "Processed Text: ['صبح', 'متوالی', 'راننده', 'تاکسی', 'اعصاب', 'جویید', 'تومانی', 'دارید', 'خرد', 'ندارید', 'تخلیه', 'غر', 'زدن', 'روز']\n"
     ]
    }
   ],
   "source": [
    "from hazm import Normalizer, WordTokenizer, stopwords_list, Lemmatizer\n",
    "import random\n",
    "import string\n",
    "\n",
    "def preprocessing(text, Apply_normalizer= True, Aplly_wordtokenizer= True, remove_stop_words= True, Apply_lemmatizer=True, remove_numbers=True):\n",
    "  '''\n",
    "  Cleaning and preprocessing the given text.\n",
    "\n",
    "  Args:\n",
    "    text (str): the input text that we want to work on in.\n",
    "    Apply_normalizer (bool): if True, shows that text will bee normalize by hazm. Defualt is True\n",
    "    Aplly_wordtokenizer (bool): if True, the text will tokenize. Defualt is True\n",
    "    remove_stop_words (list): a list that have the set of stopwords in pesian. Defualt is True\n",
    "    Apply_lemmatizer (bool): change the word to its root. Defualt is True\n",
    "    remove_numbers (bool): remove the numbers from text\n",
    "    replace the \\u200c:( text = \"می‌خوانم کتابی از کتاب‌خانه و دانش‌آموزان را می‌بینم.\"\n",
    "                        processed_text = ['می\\u200cخوانم', 'کتابی', 'از', 'کتاب\\u200cخانه', 'و', 'دانش\\u200cآموزان', 'را', 'می\\u200cبینم'])\n",
    "    string.punctuation : removes this karakters (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
    "\n",
    "  Return:\n",
    "    filtered (list): list of words(of text) after cleaning and preprocessing.\n",
    "  '''\n",
    "\n",
    "  \n",
    "  # Initinalize requiared tools from Hazm\n",
    "  normalizer= Normalizer()\n",
    "  wordtokenizer= WordTokenizer()\n",
    "  stopword_lst= stopwords_list()\n",
    "  lemmatizer = Lemmatizer()\n",
    "  panctuation= string.punctuation +  \"؟،؛.,#\"\n",
    "  \n",
    "\n",
    "\n",
    "  # Step 1 : Normalize the text\n",
    "  if Apply_normalizer:\n",
    "    text= normalizer.normalize(text)\n",
    "\n",
    "\n",
    "    \n",
    "  # Step 2: Tokenize the text to words\n",
    "  if Aplly_wordtokenizer:\n",
    "    words= wordtokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "  # Step 3: delete the aditional numbers and words\n",
    "  if remove_numbers:\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "\n",
    "\n",
    "  # step 4 : Delet the stop words from  words(list)\n",
    "  if remove_stop_words:\n",
    "    words= [word for word in words if word not in stopword_lst]\n",
    "\n",
    "\n",
    "\n",
    "  # Step 5: delete the additionala (.?!,)\n",
    "  if panctuation:\n",
    "    words= [word for word in words if word not in panctuation]  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  #step 6: find the root of words\n",
    "  if Apply_lemmatizer:\n",
    "    words= [lemmatizer.lemmatize(word, pos= 'v') for word in words]\n",
    "\n",
    "\n",
    "  #ُStep 7: replace the '\\u200c'  \n",
    "  filtered= [word.replace('\\u200c', '')for word in words]\n",
    "\n",
    "  return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Kodun ne yaptığını görmek için 😊\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x= random.randint(1, 6000)\n",
    "    sample_text = df3['clean_text'][x]\n",
    "    processed_text = preprocessing(\n",
    "        sample_text)\n",
    "    print(\"Original Text:\", sample_text)\n",
    "    print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3[\"clean_text\"] = df3[\"clean_text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان</th>\n",
       "      <th>SAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عکسی که چند روز پیش گذاشته بودم این فیلم الانشه... وسط کوه ها... لابه لای مه... #تبریز</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که همه مي خوابيدن و من خوابم نمي برد، آدما بودن اما نبودن</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>این خاک مال مردمان است نه حاکمان #ایران #مهسا_امینی</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اگه تو بغلت بودم حالم خیلی بهتر میشد</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>گاز نداریم آب نداریم برق نداریم نت نداریم پول نداریم اعصاب نداریم زندگی نداریم یه دونه داریم فعلا</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عاشقی کردنو نداریم باید بشینیم عاشقارو نگاه کنیم #تلخ</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوان ۲۹ و ۳۰ #مهسا_امینی</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگیرید ?? پاره کرد ما رو</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان  \\\n",
       "0                 عکسی که چند روز پیش گذاشته بودم این فیلم الانشه... وسط کوه ها... لابه لای مه... #تبریز    \n",
       "1     تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که همه مي خوابيدن و من خوابم نمي برد، آدما بودن اما نبودن   \n",
       "2                                                                خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه   \n",
       "3                                                     این خاک مال مردمان است نه حاکمان #ایران #مهسا_امینی   \n",
       "4                                                                    اگه تو بغلت بودم حالم خیلی بهتر میشد   \n",
       "...                                                                                                   ...   \n",
       "4918                                                    من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه   \n",
       "4919    گاز نداریم آب نداریم برق نداریم نت نداریم پول نداریم اعصاب نداریم زندگی نداریم یه دونه داریم فعلا   \n",
       "4920    یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عاشقی کردنو نداریم باید بشینیم عاشقارو نگاه کنیم #تلخ   \n",
       "4921                                  زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوان ۲۹ و ۳۰ #مهسا_امینی   \n",
       "4922                                  سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگیرید ?? پاره کرد ما رو   \n",
       "\n",
       "        SAD  \n",
       "0     OTHER  \n",
       "1       SAD  \n",
       "2     HAPPY  \n",
       "3     ANGRY  \n",
       "4       SAD  \n",
       "...     ...  \n",
       "4918  HAPPY  \n",
       "4919    SAD  \n",
       "4920    SAD  \n",
       "4921  OTHER  \n",
       "4922    SAD  \n",
       "\n",
       "[4923 rows x 2 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_excel('./train_fa2.xlsx')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دوستان اسپیس پرید چون جوزف خوابش برد ?\n",
      "OTHER\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i= random.randint(0, 4923)\n",
    "print(df4.loc[i, 'کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان'])\n",
    "print(df4.loc[i, 'SAD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حله، حالا که دیتا رو پیدا کردی، بریم مرحله به مرحله ببینیم چیکار باید بکنی. جوری توضیح میدم که انگار داری یه بازی مرحله‌ای انجام میدی و هر مرحله رو باید درست رد کنی تا بری بعدی.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **مرحله ۱: بررسی اولیه دیتاست** (مثل نگاه کردن به نقشه قبل از شروع بازی)  \n",
    "قبل از اینکه هر کاری بکنی، باید یه نگاه کلی به دیتاستت بندازی. این کار کمک می‌کنه که بفهمی:  \n",
    "- **چند تا داده داری؟** (خب گفتی ۱۳,۰۰۰ تا، اوکی)  \n",
    "- **ستون‌های دیتاست چی هستن؟** (مثلاً، ستون متن توییت، ستون برچسب احساسات و ...)  \n",
    "- **مقدار داده‌ها در هر کلاس احساسات چطوره؟** (مثلاً، نکنه ۹۰٪ توییت‌ها \"خوشحال\" باشن و فقط ۱۰٪ بقیه؟ این باعث عدم تعادل میشه)  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی اولیه:**  \n",
    "```python\n",
    "import pandas as pd  \n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # اینو با اسم فایلت عوض کن\n",
    "print(df.head())  # نمایش ۵ سطر اول\n",
    "print(df.info())  # نمایش اطلاعات کلی درباره دیتاست\n",
    "print(df['label'].value_counts())  # بررسی توزیع برچسب‌ها\n",
    "```\n",
    "✍ **اگر توزیع داده‌ها نامتعادل بود، باید روش‌هایی مثل oversampling یا undersampling رو انجام بدی (بعداً بهش می‌رسیم).**  \n",
    "\n",
    "📌 **منبع پیشنهادی برای بررسی اولیه دیتاست:**  \n",
    "[راهنمای بررسی اولیه دیتا در پانداس](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 **مرحله ۲: پاکسازی داده‌ها (Data Cleaning)** (مثل تمیز کردن صفحه بازی قبل از شروع)  \n",
    "باید مطمئن بشی که دیتای خامت مشکل خاصی نداره. مشکلات رایج اینا هستن:  \n",
    "✅ **مقادیر خالی (Missing Values)**  \n",
    "✅ **داده‌های تکراری (Duplicate Data)**  \n",
    "✅ **توییت‌های بی‌معنی (مثلاً شامل فقط @ و # و لینک‌ها بدون متن مفید)**  \n",
    "\n",
    "📌 **کد پیشنهادی برای پاکسازی:**  \n",
    "```python\n",
    "# حذف مقادیر خالی\n",
    "df = df.dropna()\n",
    "\n",
    "# حذف مقادیر تکراری\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# بررسی نمونه‌هایی که فقط شامل لینک و منشن هستن\n",
    "import re\n",
    "df = df[~df['text'].str.match(r'^\\s*(http|@|#).*$', case=False)]\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[آموزش Data Cleaning در پانداس](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 **مرحله ۳: پیش‌پردازش متن (Text Preprocessing)** (مثل تنظیمات اولیه کاراکتر توی بازی)  \n",
    "الان باید متن‌های توییت رو برای مدل آماده کنیم. توییت‌ها معمولاً کثیف هستن و پر از چیزهای اضافی مثل:  \n",
    "❌ لینک‌ها (https://...)  \n",
    "❌ نام‌های کاربری (@user)  \n",
    "❌ هشتگ‌ها (#موضوع)  \n",
    "❌ ایموجی‌ها (😂😡❤️)  \n",
    "❌ حروف اضافی کشیده (\"عاااالیییی\")  \n",
    "\n",
    "📌 **کد پیشنهادی برای تمیز کردن متن:**  \n",
    "```python\n",
    "import re  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # حذف لینک‌ها\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)  # حذف منشن‌ها\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)  # حذف هشتگ‌ها\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # حذف علائم نگارشی\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # حذف فاصله‌های اضافی\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "```\n",
    "\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای پیش‌پردازش متن](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python/)  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **مرحله ۴: تحلیل داده‌ها (Exploratory Data Analysis - EDA)** (مثل بررسی ویژگی‌های بازی قبل از شروع)  \n",
    "باید یه کم روی دیتات آنالیز انجام بدی که بفهمی:  \n",
    "- **کدوم کلمات بیشتر استفاده شدن؟**  \n",
    "- **طول جملات چقدره؟**  \n",
    "- **احساسات چقدر توزیع شدن؟**  \n",
    "\n",
    "📌 **کد پیشنهادی برای بررسی کلمات پر تکرار:**  \n",
    "```python\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[EDA در NLP](https://towardsdatascience.com/exploratory-data-analysis-eda-for-text-data-b8a26c6a00e7)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 **مرحله ۵: تبدیل متن به عدد (Tokenization & Embedding)** (مثل آماده کردن کاراکترهای بازی برای حرکت)  \n",
    "مدل‌های یادگیری ماشین فقط عدد می‌فهمن، پس باید متن رو به عدد تبدیل کنیم. روش‌های مختلفی وجود داره:  \n",
    "- **Bag of Words (BoW)** – مدل ساده‌ای که فقط تعداد کلمات رو می‌شمره  \n",
    "- **TF-IDF** – مقدار اهمیت هر کلمه در متن  \n",
    "- **Word Embeddings (مثل Word2Vec یا BERT)** – تبدیل کلمات به بردار عددی  \n",
    "\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokens = tokenizer(df[\"clean_text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[مقدمه‌ای بر Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏗 **مرحله ۶: مدل‌سازی (Training a Model)** (مثل ساختن شخصیت بازی)  \n",
    "باید تصمیم بگیری که از چه مدلی استفاده کنی:  \n",
    "✅ **مدل‌های ساده (مثل Naive Bayes, SVM)** – اگر دیتاست کوچیک باشه  \n",
    "✅ **مدل‌های پیچیده‌تر (مثل LSTM, BERT)** – اگر دیتاست بزرگ باشه و دقت بالا بخوای  \n",
    "\n",
    "📌 **اگر از مدل ساده استفاده کنی:**  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df[\"clean_text\"], df[\"label\"])\n",
    "```\n",
    "📌 **اگر از BERT استفاده کنی:**  \n",
    "[راهنمای آموزش مدل BERT برای تحلیل احساسات](https://huggingface.co/docs/transformers/training)  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **مرحله ۷: ارزیابی مدل (Evaluation & Testing)** (مثل بررسی امتیاز بازی)  \n",
    "بعد از آموزش مدل، باید ببینی چقدر خوب کار میکنه. از معیارهای **دقت (Accuracy)، دقت مثبت (Precision)، یادآوری (Recall)** استفاده کن.  \n",
    "\n",
    "📌 **کد ارزیابی مدل:**  \n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = pipeline.predict(df_test[\"clean_text\"])\n",
    "print(classification_report(df_test[\"label\"], y_pred))\n",
    "```\n",
    "📌 **منبع پیشنهادی:**  \n",
    "[راهنمای ارزیابی مدل‌های NLP](https://towardsdatascience.com/evaluation-metrics-for-text-classification-1b215e31d685)  \n",
    "\n",
    "---\n",
    "\n",
    "### **حالا تو بگو، تو کدوم مرحله سوال داری؟** 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بخاطر از دست رفتن جوانی مان ، امیدمان ، انرژی و نشاط مان..\n",
      "بخاطر خون های ریخته شده ۷۸  ۸۸  ۹۶  ۹۸ \n",
      "بخاطر مادرهایی که دل و چشمشون خونه \n",
      "بخاطر پدر مادر هایی که جوون غرق به خونشون و به خاک سپردن.\n",
      "رای بی رای \n",
      "#No2IR\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# فرض می‌کنیم که احساسات شاد با \"happy\" یا \"joyful\" مشخص می‌شن\n",
    "happy_tweets = df[df['emotion'].isin(['joy'])]\n",
    "\n",
    "# انتخاب یک ردیف تصادفی از داده‌های شاد\n",
    "i = random.randint(0, len(happy_tweets) - 1)\n",
    "print(happy_tweets.iloc[i]['tweet'])\n",
    "print(happy_tweets.iloc[i]['emotion'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i= random.randint(0, 113829)\n",
    "# print(df.loc[i, 'tweet'])\n",
    "# print(df.loc[i, 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113829 entries, 0 to 113828\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   tweet         113829 non-null  object\n",
      " 1   replyCount    113829 non-null  int64 \n",
      " 2   retweetCount  113829 non-null  int64 \n",
      " 3   likeCount     113829 non-null  int64 \n",
      " 4   quoteCount    113829 non-null  int64 \n",
      " 5   hashtags      113829 non-null  object\n",
      " 6   sourceLabel   113829 non-null  object\n",
      " 7   emotion       113829 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n",
      "emotion\n",
      "sad         30.157517\n",
      "joy         24.619385\n",
      "anger       17.630832\n",
      "fear        15.482873\n",
      "surprise    11.296770\n",
      "disgust      0.812622\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of labels\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "print(df['emotion'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Repetitive Rows\n",
    "#df= df.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
